{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edfs.firebase import ls, \\\n",
    "    mkdir, rm, read_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(\"/root/user/Consume_Price_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Consume_Price_Index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "firebase_url = 'https://dsci551-project-52d43-default-rtdb.firebaseio.com/'\n",
    "\n",
    "def seek(path):\n",
    "    url = firebase_url + path + '.json'\n",
    "    try:\n",
    "        rget = requests.get(url)\n",
    "        return rget\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        \n",
    "\n",
    "def ls(path: str) -> str:\n",
    "    '''List files under path/.\n",
    "\n",
    "    Args:\n",
    "        path (str): path starting from NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    slist = seek(f\"NameNode/{path}\")\n",
    "    rlist = slist.json()\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    if type(rlist) == dict: # iterate over rlist if rlist != None\n",
    "        for key, value in rlist.items():\n",
    "            if key == \"_\": # empty directory\n",
    "                continue\n",
    "            if type(value) == dict: # if item is folder, add '/' to the end of thge string\n",
    "                result_list.append(key + \"/\")\n",
    "            else:\n",
    "                result_list.append(key)\n",
    "        output = 'empty' if not result_list else ', '.join(result_list)\n",
    "    elif not rlist:\n",
    "        output = f'Path {path} not found'\n",
    "    else: \n",
    "        output = f'{path} is not a folder'\n",
    "    \n",
    "    return output\n",
    "\n",
    "def mkdir(path: str) -> str:\n",
    "    '''Create directory if not exists\n",
    "\n",
    "    Args:\n",
    "        path: relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    full_path = f'NameNode/{path}'\n",
    "    if seek(full_path).json() is None:\n",
    "        url = firebase_url + full_path + '.json'\n",
    "        data = '{\"_\" : \"_\"}' # empty directory\n",
    "        r = requests.put(url,data)\n",
    "        output = f'Directory {path} created'\n",
    "    else:\n",
    "        output  = 'Directory ' + path + ' already exists'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm(path: str) -> str:\n",
    "    '''Delete directory if exists\n",
    "\n",
    "    Args:\n",
    "        path: relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    full_path = f'NameNode/{path}'\n",
    "    if seek(full_path).json() is None:\n",
    "        output = 'Directory not found'\n",
    "    else:\n",
    "        url = firebase_url + full_path + '.json'\n",
    "        d = requests.delete(url)\n",
    "        if d.status_code == 200:\n",
    "            output = path + ' was succefully deleted'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPartitionLocation(file: str) -> str:\n",
    "    '''Return the locations of partitions of the\n",
    "        file\n",
    "    Args:\n",
    "        file (str): relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    path = \"NameNode/\" + file + \"/partitions\"\n",
    "    rpath = seek(path)\n",
    "    partition = requests.get(rpath.url)\n",
    "    pdict = partition.json()       \n",
    "\n",
    "    if pdict is None:\n",
    "        output = f'Partitions for {file} not found'\n",
    "    else:\n",
    "        output = json.dumps(pdict, indent=4, sort_keys=True) # Organizing the data\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readPartition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m functions \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mls\u001b[39m\u001b[39m'\u001b[39m: ls, \u001b[39m'\u001b[39m\u001b[39mmkdir\u001b[39m\u001b[39m'\u001b[39m: mkdir, \u001b[39m'\u001b[39m\u001b[39mrm\u001b[39m\u001b[39m'\u001b[39m: rm,\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mreadPartition\u001b[39m\u001b[39m'\u001b[39m: readPartition,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgetPartitionLocation\u001b[39m\u001b[39m'\u001b[39m: getPartitionLocation\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readPartition' is not defined"
     ]
    }
   ],
   "source": [
    "functions = {\n",
    "    'ls': ls, 'mkdir': mkdir, 'rm': rm,\n",
    "    'readPartition': readPartition,\n",
    "    'getPartitionLocation': getPartitionLocation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'readPartition root/user/Stats_Cap_Ind Argentina'\n",
    "input = input_text.split(' ')\n",
    "function_name = input[0]\n",
    "params = input[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions[function_name](*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Partitions for Stats_Cap_Ind not found'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPartitionLocation(\"Stats_Cap_Ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPartition(file, partition) -> str:\n",
    "    '''Return the content of partition # of\n",
    "    the specified file\n",
    "    Args:\n",
    "        file (str): relative path to NameNode/\n",
    "        partition (str):  name of the partition\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    try:\n",
    "        pdict = json.loads(getPartitionLocation(file))\n",
    "        url = pdict[partition]\n",
    "        pdict = requests.get(url).json()\n",
    "        output = json.dumps(pdict, indent=4, sort_keys=True)\n",
    "    except:\n",
    "        output = 'Partition not found'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readUrl(url) -> json:\n",
    "    return requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = readUrl(\"https://dsci551-project-52d43-default-rtdb.firebaseio.com/DataNode/Afghanistan/Stats_Cap_Ind.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"root/user/dasdasd\"\n",
    "path = \"NameNode/\" + file + \"/partitions\"\n",
    "rpath = seek(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in result.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_partitions(country_name: list, file_name: str, series_name: str):\n",
    "    list_partitions = []\n",
    "    partitions = getPartitionLocation(f\"root/user/{file_name}\")\n",
    "    partitions = json.loads(partitions)\n",
    "    for country in country_name:\n",
    "        list_partitions.append(partitions.get(country))\n",
    "    return list_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def _sum2(a, b, c):\n",
    "    return a + b\n",
    "\n",
    "def _count(a, b):\n",
    "    return a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readUrl(url, filter_year: list = [], index:str = 'year') -> json:\n",
    "\n",
    "    filter_str = ''\n",
    "    if filter_year:\n",
    "        filter_str = f\"\"\"?orderBy=\"{index}\"&startAt={filter_year[0]}&endAt={filter_year[1]}\"\"\"\n",
    "    return requests.get(f\"{url}{filter_str}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Index not defined, add \".indexOn\": \"year\", for path \"/DataNode/Afghanistan/Stats_Cap_Ind\", to the rules'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readUrl('https://dsci551-project-52d43-default-rtdb.firebaseio.com/DataNode/Afghanistan/Stats_Cap_Ind.json', [2005, 2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_partitions(partitions: list):\n",
    "    list_of_result = []\n",
    "    for partition in partitions:\n",
    "        # print(partition)\n",
    "        # result = sum(partition)\n",
    "        result = reduce(_sum, partition)\n",
    "        list_of_result.append(result)\n",
    "    return list_of_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 4]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_partitions([[1,2,3,1], [1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce(_sum2, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(_count, [[1,2,3,1], [1,1,1,1]], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dsci551-project-52d43-default-rtdb.firebaseio.com/DataNode/Afghanistan/Stats_Cap_Ind.json',\n",
       " 'https://dsci551-project-52d43-default-rtdb.firebaseio.com/DataNode/Azerbaijan/Stats_Cap_Ind.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_partitions([\"Afghanistan\", \"Azerbaijan\"], 'Stats_Cap_Ind', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a = json.dumps(result, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"root/user/Stats_Cap_Ind\"\n",
    "result = getPartitionLocation(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 =json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'root/daniel was succefully deleted'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm(\"root/daniel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Directory data2/China already exists'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkdir(\"data2/China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ls dsdadsada dasd\"\n",
    "input = input_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ls root/daniel/test2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "functions = {'ls': ls, 'mkdir': mkdir, 'rm': rm}\n",
    "\n",
    "input = input_text.split(' ')\n",
    "function_name = input[0]\n",
    "params = input[1:]\n",
    "\n",
    "try:\n",
    "    output = functions[function_name](*params)\n",
    "except KeyError:\n",
    "    output = \"Command not found. Valid commands: ls, mkdir, rm, put, getPartitionLocations, readPartition\"\n",
    "except TypeError:\n",
    "    output = \"Please verify function required arguments\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[function_name](*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Data_Extract_From_Statistical_Capacity_Indicators/42377300-c075-4554-a55f-41cd64c79126_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(500).to_csv('datasets/Stats_Cap_Ind_Sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 08:02:03 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 262682 ms exceeds timeout 120000 ms\n",
      "22/11/20 08:02:03 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# function to get year columns\n",
    "def is_year (c):\n",
    "    return any(char.isdigit() for char in c)\n",
    "\n",
    "# change columns names\n",
    "new_columns = list()\n",
    "columns = df.columns\n",
    "for c in columns:\n",
    "    if is_year(c):\n",
    "        new_columns.append(c[:4])\n",
    "    else:\n",
    "        new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "# change column names in dataframe\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = df.melt(id_vars=[\"Country_Name\", \"Series_Name\"], \n",
    "        var_name=\"Year\", \n",
    "        value_name=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['Year'] = df_melted['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = df_melted.loc[df_melted['Value'] != '..'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.00000\n",
       "1         0.00000\n",
       "2         1.00000\n",
       "3         0.66667\n",
       "4         1.00000\n",
       "           ...   \n",
       "75917     0.00000\n",
       "75918    50.00000\n",
       "75919     0.00000\n",
       "75920     0.00000\n",
       "75921     0.00000\n",
       "Name: Value, Length: 69959, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted.loc[df_melted['Value'] != '..']['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['Value'] = df_melted['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Name</th>\n",
       "      <th>Series_Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8935</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13401</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22333</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26799</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31265</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35731</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40197</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44663</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49129</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53595</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58061</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62527</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66993</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71459</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country_Name         Series_Name  Year    Value\n",
       "3      Afghanistan  Child malnutrition  2020  0.66667\n",
       "4469   Afghanistan  Child malnutrition  2019  0.33333\n",
       "8935   Afghanistan  Child malnutrition  2018  0.33333\n",
       "13401  Afghanistan  Child malnutrition  2017  0.33333\n",
       "17867  Afghanistan  Child malnutrition  2016  0.33333\n",
       "22333  Afghanistan  Child malnutrition  2015  0.33333\n",
       "26799  Afghanistan  Child malnutrition  2014  0.33333\n",
       "31265  Afghanistan  Child malnutrition  2013  0.33333\n",
       "35731  Afghanistan  Child malnutrition  2012  0.33333\n",
       "40197  Afghanistan  Child malnutrition  2011  0.33333\n",
       "44663  Afghanistan  Child malnutrition  2010  0.66667\n",
       "49129  Afghanistan  Child malnutrition  2009  0.66667\n",
       "53595  Afghanistan  Child malnutrition  2008  0.66667\n",
       "58061  Afghanistan  Child malnutrition  2007  0.33333\n",
       "62527  Afghanistan  Child malnutrition  2006  0.33333\n",
       "66993  Afghanistan  Child malnutrition  2005  0.33333\n",
       "71459  Afghanistan  Child malnutrition  2004  0.33333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted.loc[\n",
    "    (df_melted['Country_Name'] == 'Afghanistan') & (df_melted['Series_Name'] == 'Child malnutrition')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Name</th>\n",
       "      <th>Series_Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Agricultural census</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Balance of payments manual in use</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child malnutrition</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Child mortality</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71480</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71481</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Source data assessment of statistical capacity...</td>\n",
       "      <td>2004</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71482</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Special Data Dissemination Standard</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71483</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>UNESCO reporting</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71484</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Vital registration system coverage</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>493 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country_Name                                        Series_Name  Year  \\\n",
       "0      Afghanistan                                    Access to water  2020   \n",
       "1      Afghanistan                                Agricultural census  2020   \n",
       "2      Afghanistan                  Balance of payments manual in use  2020   \n",
       "3      Afghanistan                                 Child malnutrition  2020   \n",
       "4      Afghanistan                                    Child mortality  2020   \n",
       "...            ...                                                ...   ...   \n",
       "71480  Afghanistan                                 Primary completion  2004   \n",
       "71481  Afghanistan  Source data assessment of statistical capacity...  2004   \n",
       "71482  Afghanistan                Special Data Dissemination Standard  2004   \n",
       "71483  Afghanistan                                   UNESCO reporting  2004   \n",
       "71484  Afghanistan                 Vital registration system coverage  2004   \n",
       "\n",
       "         Value  \n",
       "0            1  \n",
       "1            0  \n",
       "2            1  \n",
       "3      0.66667  \n",
       "4            1  \n",
       "...        ...  \n",
       "71480        0  \n",
       "71481       20  \n",
       "71482        0  \n",
       "71483        0  \n",
       "71484        0  \n",
       "\n",
       "[493 rows x 4 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted.loc[\n",
    "    (df_melted['Country_Name'] == 'Afghanistan')]# & (df_melted['Series_Name'] == 'Child malnutrition')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (df_melted.Country_Name.isin(['Albania']))\n",
    "    & (df_melted.Series_Name == 'Access to water')\n",
    "    & (df_melted.Year >= 2000)\n",
    "    & (df_melted.Year <= 2020)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "filtered_dataset = df_melted[mask].copy()\n",
    "for country_name in filtered_dataset.Country_Name.unique():\n",
    "    df_temp = filtered_dataset[filtered_dataset['Country_Name'] == country_name]\n",
    "    dict_append = {\n",
    "        \"x\": df_temp[\"Year\"],\n",
    "        \"y\": df_temp[\"Value\"],\n",
    "        \"type\": \"lines\",\n",
    "        \"hovertemplate\": \"%{y:.2f}<extra></extra>\",\n",
    "        \"name\": country_name\n",
    "    }\n",
    "    data_list.append(dict_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value\n",
       "Year       \n",
       "2004    1.0\n",
       "2005    1.0\n",
       "2006    1.0\n",
       "2007    1.0\n",
       "2008    1.0\n",
       "2009    1.0\n",
       "2010    1.0\n",
       "2011    1.0\n",
       "2012    1.0\n",
       "2013    1.0\n",
       "2014    1.0\n",
       "2015    1.0\n",
       "2016    1.0\n",
       "2017    1.0\n",
       "2018    1.0\n",
       "2019    1.0\n",
       "2020    1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted.loc[df_melted['Series_Name'] == 'Access to water'].groupby('Year').agg({'Value': 'median'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Name</th>\n",
       "      <th>Series_Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4466</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13398</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17864</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22330</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26796</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31262</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35728</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40194</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44660</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49126</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53592</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58058</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62524</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66990</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71456</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Access to water</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country_Name      Series_Name  Year Value\n",
       "0      Afghanistan  Access to water  2020     1\n",
       "4466   Afghanistan  Access to water  2019     1\n",
       "8932   Afghanistan  Access to water  2018     1\n",
       "13398  Afghanistan  Access to water  2017     1\n",
       "17864  Afghanistan  Access to water  2016     1\n",
       "22330  Afghanistan  Access to water  2015     1\n",
       "26796  Afghanistan  Access to water  2014     1\n",
       "31262  Afghanistan  Access to water  2013     1\n",
       "35728  Afghanistan  Access to water  2012     1\n",
       "40194  Afghanistan  Access to water  2011     1\n",
       "44660  Afghanistan  Access to water  2010     1\n",
       "49126  Afghanistan  Access to water  2009     1\n",
       "53592  Afghanistan  Access to water  2008     1\n",
       "58058  Afghanistan  Access to water  2007     1\n",
       "62524  Afghanistan  Access to water  2006     1\n",
       "66990  Afghanistan  Access to water  2005     1\n",
       "71456  Afghanistan  Access to water  2004     1"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted.loc[(df_melted.Country_Name == 'Afghanistan')\n",
    "        & (df_melted.Series_Name == 'Access to water')\n",
    "        & (df_melted.Year >= 2000)\n",
    "        & (df_melted.Year <= 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.3.1/libexec/bin/spark-class: line 71: /Library/java/JavaVirtualMachines/adoptopenjdk-8.jdk/contents/Home//bin/java: No such file or directory\n",
      "/usr/local/Cellar/apache-spark/3.3.1/libexec/bin/spark-class: line 96: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"datasets/Data_Extract_From_Statistical_Capacity_Indicators/42377300-c075-4554-a55f-41cd64c79126_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as ccnx\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#python connector setup\n",
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)\n",
    "\n",
    "####################\n",
    "# Helper Functions #\n",
    "####################\n",
    "\n",
    "def Sort_Tuple(tup: list, idx: int) -> list:\n",
    "    '''\n",
    "    Return the sorted list of tuples by the element of the tuple\n",
    "    Args:\n",
    "        tup (list): unsorted list of tuples\n",
    "        idx (int): the element of the tuple to sort by\n",
    "    Returns:\n",
    "        (list) Sorted list of tuples\n",
    "    '''\n",
    "    return(sorted(tup, key = lambda x: x[idx]))\n",
    "\n",
    "def key_idx(str_list):\n",
    "    '''\n",
    "    Return the index of 'Country Name' if it exists in the dataset, and 0 if\n",
    "    the column name is not present\n",
    "    Args:\n",
    "        str_list (list): the list of column names\n",
    "    Returns:\n",
    "        (int) the index returned\n",
    "    '''\n",
    "    try:\n",
    "        return str_list.index('Country Name')\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def key_cleaning(row):\n",
    "    #key cleaning\n",
    "    clean_key =  re.sub(r'[^A-Za-z0-9 ]+', '', row[0]).replace(\" \", \"_\")\n",
    "    key = clean_key if clean_key != \"\" else \"invalid_key\"\n",
    "    if key.isnumeric() and key.length() > 0:\n",
    "        key = f\"t{key}\"\n",
    "    if len(key) >= 64:\n",
    "        key = key[0:40]\n",
    "    return key\n",
    "\n",
    "####################\n",
    "# API Functions    #\n",
    "####################\n",
    "\n",
    "def read_dataset(path):\n",
    "    # (partition_name, csv_index, comma-separated-string)\n",
    "    list_of_tuples = getPartitionData(path)\n",
    "    list_of_lists = [tuple[2].split(\",\") for tuple in list_of_tuples]\n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header #set the header row as the df header\n",
    "\n",
    "    df = df.drop([\"Country Code\", \"Series Code\"], \"columns\")\n",
    "\n",
    "    df_melted = df.melt(id_vars=[\"Country Name\", \"Series Name\"],\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\")\n",
    "\n",
    "    df_melted[\"Year\"] = df_melted[\"Year\"].str[0:4]\n",
    "\n",
    "    df_melted = df_melted.loc[df_melted.Value.str.isnumeric()].copy()\n",
    "\n",
    "    # change columns names\n",
    "    new_columns = list()\n",
    "    columns = df_melted.columns\n",
    "    for c in columns:\n",
    "            new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "    # change column names in dataframe\n",
    "    df_melted.columns = new_columns\n",
    "\n",
    "    return df_melted.astype({'Year':'int', 'Value': 'float'})\n",
    "\n",
    "def seek(path):\n",
    "    '''\n",
    "    Returns the filestructure that matches the specified path\n",
    "    Args:\n",
    "        str_list (list): the list of column names\n",
    "    Returns:\n",
    "        (int) the index returned\n",
    "    '''\n",
    "    seek_statement = \"SELECT * FROM df WHERE path = %s\"\n",
    "    mycursor.execute(seek_statement, (path,))\n",
    "    myresult = mycursor.fetchall()\n",
    "    return myresult\n",
    "\n",
    "def mkdir(path, name):\n",
    "    '''\n",
    "    Create the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            pathname = f\"{path}/{name}\"\n",
    "            dup_result = seek(pathname)\n",
    "            if not dup_result:\n",
    "                insert_statement = \"INSERT INTO df VALUES (%s, 'DIRECTORY')\"\n",
    "                mycursor.execute(insert_statement, (pathname,))\n",
    "                mydb.commit()\n",
    "                output = f\"directory {name} created\"\n",
    "            else:\n",
    "                output = \"directory already exists\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def rm(path, name):\n",
    "    '''\n",
    "    Removes the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    filepath =  f\"{path}/{name}\"\n",
    "    if result:\n",
    "        select_statement = \"SELECT * FROM df WHERE path LIKE %s\"\n",
    "        mycursor.execute(select_statement, (filepath + \"%\",))\n",
    "        result = mycursor.fetchall()\n",
    "        if len(result) != 1:\n",
    "            output = \"invalid deletion\"\n",
    "        else:\n",
    "            delete_statement = \"DELETE FROM df WHERE path LIKE %s\"\n",
    "            mycursor.execute(delete_statement, (filepath,)) #TODO: adding % here will add -r functionality\n",
    "            mydb.commit()\n",
    "            output = f\"{filepath} deleted\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {filepath}\"\n",
    "    return output\n",
    "\n",
    "def ls(path):\n",
    "    '''\n",
    "    Returns the contents of the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"FILE\":\n",
    "            output = \"Cannot run 'ls' on files\"\n",
    "        elif result[0][1] == \"DIRECTORY\":\n",
    "            ls_statement = \"SELECT * FROM df WHERE path REGEXP %s\"\n",
    "            mycursor.execute(ls_statement, (f\"^{path}\\/[^\\/]+$\",))\n",
    "            output = mycursor.fetchall()\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def getPartitionLocations(path):\n",
    "    '''\n",
    "    Returns the blockLocations that match the file at the specified\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the blockLocations in a list of tuples\n",
    "    '''\n",
    "    cat_statement = \"SELECT * FROM blockLocations WHERE path = %s\"\n",
    "    mycursor.execute(cat_statement, (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    return result\n",
    "\n",
    "def readPartition(path, partition_name):\n",
    "    '''\n",
    "    Returns the contents of a specified partition_name\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        partition_name: the name of the partition\n",
    "    Returns:\n",
    "        [list of (tuple)]: the data contents of the partition\n",
    "    '''\n",
    "    mycursor.execute(f\"SELECT * FROM {partition_name} WHERE path = %s\", (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    partition_data = []\n",
    "    for line in result:\n",
    "        partition_data.append((partition_name, line[1], line[2]))\n",
    "    return partition_data\n",
    "\n",
    "def cat(path):\n",
    "    '''\n",
    "    Returns the contents the file at the specified path\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the text of the file\n",
    "    '''\n",
    "    output = \"\"\n",
    "    sorted_data_list = getPartitionData(path)\n",
    "    for s in sorted_data_list:\n",
    "        output += s[2] +\"\\n\"\n",
    "    return output\n",
    "\n",
    "def getPartitionData(path):\n",
    "    '''\n",
    "    Returns the contents the file at the specified path\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the text of the file\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            output = \"Cannot run 'cat' on directories\"\n",
    "        elif result[0][1] == \"FILE\":\n",
    "            myresult = getPartitionLocations(path)\n",
    "            data_list = []\n",
    "            for partition in myresult:\n",
    "                data_list = data_list + readPartition(path, partition[1])\n",
    "            sorted_data_list = Sort_Tuple(data_list, 1)\n",
    "            return sorted_data_list\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def put(path, name, csv):\n",
    "    '''\n",
    "    places the file from a local directory into the EDFS\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        name (str): the name of the file to be created\n",
    "        csv (str): the path to the csv file to be placed\n",
    "    Returns:\n",
    "        output (str): the success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            dup_result = seek(f\"{path}/{name}\")\n",
    "            if not dup_result:\n",
    "                hash_lists = hash(path, name, csv)\n",
    "                output = f\"file {name} created\"\n",
    "            else:\n",
    "                output = \"file already exists\"\n",
    "        else:\n",
    "            output = \"cannot place a file in a file\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def hash(path, name, csv_file):\n",
    "    '''\n",
    "    Alters the metadata to allocate new datanotes if needed and places the file\n",
    "    data into the nodes\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        name (str): the name of the file to be created\n",
    "        csv (str): the path to the csv file to be placed\n",
    "    Returns:\n",
    "        key_list (list): the list of keys to datanodes that have been\n",
    "        allocated to\n",
    "    '''\n",
    "\n",
    "    #execute metadata alter\n",
    "    meta_statement = \"INSERT INTO df VALUES (%s, 'FILE');\"\n",
    "    mycursor.execute(meta_statement, (f\"{path}/{name}\",))\n",
    "    mydb.commit()\n",
    "    with open(csv_file) as f:\n",
    "\n",
    "        key_list, csv_counter = {}, 0\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        header = next(reader)\n",
    "        key_index = key_idx(header)\n",
    "        key = key_cleaning(header)\n",
    "\n",
    "        #TODO: modularize this create code JFC\n",
    "        try:\n",
    "            create_statement = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {key}(\n",
    "                    path varchar(255),\n",
    "                    data_index int,\n",
    "                    data text,\n",
    "                    FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                )\"\"\"\n",
    "            insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "            mycursor.execute(create_statement)\n",
    "            mydb.commit()\n",
    "            mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(header)))\n",
    "            mydb.commit()\n",
    "            key_list[key] = None\n",
    "            csv_counter += 1\n",
    "        except:\n",
    "            output = f\"ERROR: {mycursor.statement}\"\n",
    "\n",
    "        for row in reader:\n",
    "            key = key_cleaning(row)\n",
    "            #try insert data into datanode\n",
    "            try:\n",
    "                create_statement = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {key}(\n",
    "                        path varchar(255),\n",
    "                        data_index int,\n",
    "                        data text,\n",
    "                        FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                    )\"\"\"\n",
    "                insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "                mycursor.execute(create_statement)\n",
    "                mydb.commit()\n",
    "                mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(row)))\n",
    "                mydb.commit()\n",
    "                key_list[key] = None\n",
    "                csv_counter += 1\n",
    "            except:\n",
    "                output = f\"ERROR: {mycursor.statement}\"\n",
    "                rm(path, name)\n",
    "                # return output\n",
    "\n",
    "        #write data into datanodes\n",
    "        for key in key_list.keys():\n",
    "             block_statement = \"INSERT INTO blockLocations VALUES(%s, %s);\"\n",
    "             mycursor.execute(block_statement, (f\"{path}/{name}\", key))\n",
    "        mydb.commit()\n",
    "        return key_list\n",
    "\n",
    "######################\n",
    "# Database Functions #\n",
    "######################\n",
    "\n",
    "def delete(list):\n",
    "    '''\n",
    "    Drops all tables in the list from the edfs\n",
    "    Args:\n",
    "        list (list): the list of table names to drop\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        for item in list:\n",
    "            drop_table = f\"DROP TABLE {key}\"\n",
    "            mycursor.execute(drop_table)\n",
    "            mydb.commit()\n",
    "        return \"Dropped tables\"\n",
    "    except:\n",
    "        return \"Database drop error\"\n",
    "\n",
    "def new_env(edfs):\n",
    "    '''\n",
    "    Executes EDFS SQL setup queries\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystel to setup\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    env_statements = [\n",
    "            f\"CREATE DATABASE {edfs}\",\n",
    "            f\"USE {edfs}\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE df (\n",
    "                    path varchar(255),\n",
    "                    type varchar(255),\n",
    "                    PRIMARY KEY(path)\n",
    "                )\"\"\",\n",
    "            \"INSERT INTO df VALUES ('/root', 'DIRECTORY')\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE blockLocations (\n",
    "                    path varchar(255),\n",
    "                    partition_name varchar(255),\n",
    "                    CONSTRAINT FOREIGN KEY (path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                )\"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        for s in env_statements:\n",
    "            mycursor.execute(s)\n",
    "        mydb.commit()\n",
    "        return f\"{edfs} created\"\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "\n",
    "def delete_env(edfs):\n",
    "    '''\n",
    "    Drops the EDFS database entirely\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystel to drop\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        drop_database = f\"DROP DATABASE {edfs};\"\n",
    "        mycursor.execute(drop_database)\n",
    "        mydb.commit()\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} deleted\"\n",
    "\n",
    "def start_env(edfs):\n",
    "    '''\n",
    "    Uses EDFS database\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystem to run\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        use_database = f\"USE {edfs}\"\n",
    "        mycursor.execute(use_database)\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} started\"\n",
    "\n",
    "# def test_edfs(argv):\n",
    "\n",
    "#     edfs = \"edfs\"\n",
    "\n",
    "#     #Testing\n",
    "#     if \"--delete\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#     elif \"--new\" in argv:\n",
    "#         print(new_env(edfs))\n",
    "#     elif \"--restart\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#         print(new_env(edfs))\n",
    "#     else:\n",
    "#         print(start_env(edfs))\n",
    "#         print(mkdir(\"/root\", \"foo\"))\n",
    "#         print(mkdir(\"/root/foo\", \"bar\"))\n",
    "#         #todo: put check to make sure that the file source exists\n",
    "#         print(put(\"/root/foo\", \"data\", \"../datasets/sql-edfs/data.csv\"))\n",
    "#         print(cat(\"/root/foo/data\"))\n",
    "#         print(ls(\"/root/foo\"))\n",
    "#         print(rm(\"/root\", \"data\"))\n",
    "#         print(ls(\"/tree\"))\n",
    "\n",
    "\n",
    "#     # test_edfs(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Database error'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edfs=\"edfs\"\n",
    "delete_env(edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_dataset() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 61\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df2 \u001b[39m=\u001b[39m read_dataset(mycursor, \u001b[39m\"\u001b[39;49m\u001b[39m/root/foo/data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: read_dataset() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "df2 = read_dataset(mycursor, \"/root/foo/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Series Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>1990</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>1990</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>1990</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>1990</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>1990</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country Name                              Series Name  Year  Value\n",
       "7591      Albania  Access to electricity (% of population)  1990  100.0\n",
       "7594      Andorra  Access to electricity (% of population)  1990  100.0\n",
       "7599        Aruba  Access to electricity (% of population)  1990  100.0\n",
       "7600    Australia  Access to electricity (% of population)  1990  100.0\n",
       "7601      Austria  Access to electricity (% of population)  1990  100.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 67\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y146sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m df[\u001b[39m~\u001b[39mdf[\u001b[39m'\u001b[39;49m\u001b[39mValue\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39misnull()]\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "df = df[~df['Value'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7591     100.0\n",
       "7594     100.0\n",
       "7599     100.0\n",
       "7600     100.0\n",
       "7601     100.0\n",
       "         ...  \n",
       "15379    100.0\n",
       "15380    100.0\n",
       "15381    100.0\n",
       "15393    100.0\n",
       "15418    100.0\n",
       "Name: Value, Length: 2739, dtype: float64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[df2.Value.str.isnumeric()]['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         AFG\n",
       "1         ALB\n",
       "2         DZA\n",
       "3         ASM\n",
       "4         AND\n",
       "         ... \n",
       "17285    None\n",
       "17286    None\n",
       "17287    None\n",
       "17288    None\n",
       "17289    None\n",
       "Name: Value, Length: 17290, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df['Series Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #python connector setup\n",
    "    mydb = ccnx.connect(\n",
    "      host=\"localhost\",\n",
    "      user=\"root\",\n",
    "      password=sys.argv[1],\n",
    "    )\n",
    "    mycursor = mydb.cursor(buffered=True)\n",
    "\n",
    "    test_edfs(mycursor, sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs created'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_env(mycursor,edfs)\n",
    "new_env(mycursor,edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Invalid path: /root/foo'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop_database = f\"DROP DATABASE {edfs};\"\n",
    "# mycursor.execute(drop_database)\n",
    "# mycursor.commit()\n",
    "put(mycursor, \"/root/foo\", \"data\", \"datasets/sql-edfs/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getPartitionLocations(mycursor, \"/root/foo/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file data created'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkdir(mycursor, \"/root\", \"foo\")\n",
    "mkdir(mycursor, \"/root/foo\", \"bar\")\n",
    "#todo: put check to make sure that the file source exists\n",
    "put(mycursor, \"/root/foo\", \"data\", \"datasets/sql-edfs/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put(mycursor, \"/root/foo\", \"data\", \"datasets/sql-edfs/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edfs = \"edfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Database error'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_env(mycursor, edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 78\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(cat(mycursor, \u001b[39m\"\u001b[39;49m\u001b[39m/root/foo/data\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "print(cat(mycursor, \"/root/foo/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "# print(mkdir(mycursor, \"/root\", \"foo\"))\n",
    "# print(mkdir(mycursor, \"/root/foo\", \"bar\"))\n",
    "#todo: put check to make sure that the file source exists\n",
    "print(put(mycursor, \"/root/foo\", \"data\", \"datasets/sql-edfs/data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName('DataLake_Ingestion').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" org.apache.spark.SparkException: Cluster deploy mode is currently not supported for python applications on standalone clusters.\n",
      "\tat org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:273)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mspark://3.86.208.117:7077\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.shuffle.service.enabled\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfalse\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.dynamicAllocation.enabled\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfalse\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.submit.deployMode\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcluster\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m5g\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m5g\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.instances\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.cores\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y111sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://3.86.208.117:7077\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    # .config(\"spark.submit.deployMode\", \"cluster\")\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "        # .config(\"spark.driver.host\", \"3.86.208.117\")\\\n",
    "    # .config(\"spark.driver.bindAddress\", \"3.86.208.117\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = spark.sparkContext.parallelize([1,2,1,1,11])\n",
    "rdd = sc.parallelize([1,2,1,1,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edfs.firebase as file_system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file_system.read_dataset(f\"root/user/Stats_Cap_Ind_Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Name</th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Series_Name</th>\n",
       "      <th>Series_Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.66667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2013</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2011</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2009</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2007</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Primary completion</td>\n",
       "      <td>5.51.01.08.primcomp</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>National accounts base year</td>\n",
       "      <td>2.01.01.02.nabase</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_Name Country_Code                  Series_Name  \\\n",
       "0   Afghanistan          AFG           Primary completion   \n",
       "1   Afghanistan          AFG  National accounts base year   \n",
       "2   Afghanistan          AFG           Primary completion   \n",
       "3   Afghanistan          AFG  National accounts base year   \n",
       "4   Afghanistan          AFG           Primary completion   \n",
       "5   Afghanistan          AFG  National accounts base year   \n",
       "6   Afghanistan          AFG           Primary completion   \n",
       "7   Afghanistan          AFG  National accounts base year   \n",
       "8   Afghanistan          AFG           Primary completion   \n",
       "9   Afghanistan          AFG  National accounts base year   \n",
       "10  Afghanistan          AFG           Primary completion   \n",
       "11  Afghanistan          AFG  National accounts base year   \n",
       "12  Afghanistan          AFG           Primary completion   \n",
       "13  Afghanistan          AFG  National accounts base year   \n",
       "14  Afghanistan          AFG           Primary completion   \n",
       "15  Afghanistan          AFG  National accounts base year   \n",
       "16  Afghanistan          AFG           Primary completion   \n",
       "17  Afghanistan          AFG  National accounts base year   \n",
       "18  Afghanistan          AFG           Primary completion   \n",
       "19  Afghanistan          AFG  National accounts base year   \n",
       "20  Afghanistan          AFG           Primary completion   \n",
       "21  Afghanistan          AFG  National accounts base year   \n",
       "22  Afghanistan          AFG           Primary completion   \n",
       "23  Afghanistan          AFG  National accounts base year   \n",
       "24  Afghanistan          AFG           Primary completion   \n",
       "25  Afghanistan          AFG  National accounts base year   \n",
       "26  Afghanistan          AFG           Primary completion   \n",
       "27  Afghanistan          AFG  National accounts base year   \n",
       "28  Afghanistan          AFG           Primary completion   \n",
       "29  Afghanistan          AFG  National accounts base year   \n",
       "30  Afghanistan          AFG           Primary completion   \n",
       "31  Afghanistan          AFG  National accounts base year   \n",
       "32  Afghanistan          AFG           Primary completion   \n",
       "33  Afghanistan          AFG  National accounts base year   \n",
       "\n",
       "            Series_Code  Year    Value  \n",
       "0   5.51.01.08.primcomp  2020  0.66667  \n",
       "1     2.01.01.02.nabase  2020  1.00000  \n",
       "2   5.51.01.08.primcomp  2019  0.00000  \n",
       "3     2.01.01.02.nabase  2019  0.00000  \n",
       "4   5.51.01.08.primcomp  2018  0.00000  \n",
       "5     2.01.01.02.nabase  2018  0.00000  \n",
       "6   5.51.01.08.primcomp  2017  0.00000  \n",
       "7     2.01.01.02.nabase  2017  0.00000  \n",
       "8   5.51.01.08.primcomp  2016  0.00000  \n",
       "9     2.01.01.02.nabase  2016  0.00000  \n",
       "10  5.51.01.08.primcomp  2015  0.00000  \n",
       "11    2.01.01.02.nabase  2015  0.00000  \n",
       "12  5.51.01.08.primcomp  2014  0.00000  \n",
       "13    2.01.01.02.nabase  2014  1.00000  \n",
       "14  5.51.01.08.primcomp  2013  0.00000  \n",
       "15    2.01.01.02.nabase  2013  1.00000  \n",
       "16  5.51.01.08.primcomp  2012  0.00000  \n",
       "17    2.01.01.02.nabase  2012  1.00000  \n",
       "18  5.51.01.08.primcomp  2011  0.33333  \n",
       "19    2.01.01.02.nabase  2011  1.00000  \n",
       "20  5.51.01.08.primcomp  2010  0.33333  \n",
       "21    2.01.01.02.nabase  2010  1.00000  \n",
       "22  5.51.01.08.primcomp  2009  0.33333  \n",
       "23    2.01.01.02.nabase  2009  1.00000  \n",
       "24  5.51.01.08.primcomp  2008  0.33333  \n",
       "25    2.01.01.02.nabase  2008  1.00000  \n",
       "26  5.51.01.08.primcomp  2007  0.33333  \n",
       "27    2.01.01.02.nabase  2007  1.00000  \n",
       "28  5.51.01.08.primcomp  2006  0.00000  \n",
       "29    2.01.01.02.nabase  2006  1.00000  \n",
       "30  5.51.01.08.primcomp  2005  0.00000  \n",
       "31    2.01.01.02.nabase  2005  1.00000  \n",
       "32  5.51.01.08.primcomp  2004  0.00000  \n",
       "33    2.01.01.02.nabase  2004  0.00000  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as ccnx\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "########################\n",
    "# mongodb client setup #\n",
    "########################\n",
    "\n",
    "client = MongoClient('mongodb://root:root@localhost:27017/')\n",
    "db = client['edfs']\n",
    "# db = new Mongo().getDB(\"edfs\")\n",
    "#collection = db['name_of_collection']\n",
    "\n",
    "#python connector setup\n",
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs_mongo\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simplejson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 89\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y303sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y303sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpymongo\u001b[39;00m \u001b[39mimport\u001b[39;00m MongoClient\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y303sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msimplejson\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y303sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y303sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simplejson'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import simplejson as sp\n",
    "import json\n",
    "import os\n",
    "\n",
    "#mongodb client setup\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['edfs']\n",
    "#collection = db['name_of_collection']\n",
    "\n",
    "#python connector setup\n",
    "\n",
    "####################\n",
    "# Helper Functions #\n",
    "####################\n",
    "\n",
    "def Sort_Tuple(tup: list, idx: int) -> list:\n",
    "    return(sorted(tup, key = lambda x: x[idx]))\n",
    "\n",
    "def key_idx(str_list):\n",
    "    try:\n",
    "        return str_list.index('Country Name')\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def key_cleaning(row):\n",
    "    #key cleaning\n",
    "    clean_key =  re.sub(r'[^A-Za-z0-9 ]+', '', row[0]).replace(\" \", \"_\")\n",
    "    key = clean_key if clean_key != \"\" else \"invalid_key\"\n",
    "    if key.isnumeric() and key.length() > 0:\n",
    "        key = f\"t{key}\"\n",
    "    if len(key) >= 64:\n",
    "        key = key[0:40]\n",
    "    return key\n",
    "\n",
    "####################\n",
    "# API Functions    #\n",
    "####################\n",
    "\n",
    "def read_dataset(path):\n",
    "    df=pd.read_csv(path)\n",
    "    df=df.reset_index()\n",
    "    df_melted = df.melt(df.set_index('Country Name'))\n",
    "    new_columns = list()\n",
    "    columns = df_melted.columns.str.strip()\n",
    "    for c in columns:\n",
    "            new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "    # change column names in dataframe\n",
    "    df_melted.columns = new_columns\n",
    "    df_melted.columns = df_melted.columns.astype(str)\n",
    "    #print(\"readDataset\", df_melted.astype({'Year':'int', 'Value': 'float'}))\n",
    "    return df_melted\n",
    "\n",
    "def seek(path):\n",
    "    #print(\"seek\", db.blockLocations.find_one({\"path\": path}))\n",
    "    return db.blockLocations.find_one({\"Path\": path})\n",
    "\n",
    "def mkdir(path, name):\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    #print(\"mkdir\", result)\n",
    "    if not result:\n",
    "        path = path + \"/\" + name\n",
    "        db.blockLocations.insert_one({\"Path\": path, \"type\" : 'DIRECTORY'})\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    print(\"mkdir\",output)\n",
    "    #return output\n",
    "\n",
    "def rm(path, name):\n",
    "    result = seek(path)\n",
    "    path =  f\"{path}/{name}\"\n",
    "    output = \"\"\n",
    "    if result:\n",
    "        db.blockLocations.insert_one({\"Path\": path, \"type\" : 'DIRECTORY'})\n",
    "        output = f\"{path} deleted\"\n",
    "    else:\n",
    "        output = \"invalid deletion\"\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def ls(path):\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[\"type\"] == \"FILE\":\n",
    "            output = \"Cannot run 'ls' on files\"\n",
    "        elif result[\"type\"] == \"DIRECTORY\":\n",
    "            #output1=db.df.find()\n",
    "            query = {\"path\": {\"$regex\": f\"{path}\",\"$options\" :'i'}}\n",
    "            output1=db.blockLocations.find(query)\n",
    "            final=[]\n",
    "            for o in output1:\n",
    "                final.append(o)\n",
    "    else:\n",
    "        final = f\"Invalid path: {path}\"\n",
    "    \n",
    "    if(len(final)==0):\n",
    "        return final\n",
    "    else:\n",
    "        return final\n",
    "\n",
    "def readPartition(inp,file,path):\n",
    "    #a=put(file,path)\n",
    "    path1= '/'+path+'/'+file+'/'+inp\n",
    "    X = db.blockLocations.find_one({\"path\":path1})\n",
    "    if(X):\n",
    "        return (\"FILE EXISTS\")\n",
    "    else:\n",
    "        return (\"FILE DOES NOT EXIST\")\n",
    "\n",
    "def cat(path):\n",
    "    db.blockLocations.aggregate([\n",
    "  { \"$project\": { \"path\": { \"$concat\": [ \"$path\", \" - \", \"$type\" ] } } },\n",
    "  { \"$merge\": \"Concatenate\" }\n",
    "])\n",
    "    return(\"Concatenated\")\n",
    "\n",
    "def put(path, name, csvf):\n",
    "    header = [ \"\\ufeffCountry Name\",\t\"Country Code\",\t\"Indicator Name\",\n",
    "    \t\"2003\",\t\"2004\",\t\"2005\",\t\"2006\",\t\"2007\",\t\"2008\",\t\"2009\",\t\n",
    "        \"2010\",\t\"2011\",\t\"2012\",\t\"2013\",\t\"2014\",\t\"2015\",\t\"2016\",\n",
    "        \"2017\",\t\"2018\",\t\"2019\",\t\"2020\",\t\"2021\"]\n",
    "    csvpath = path+csvf\n",
    "    csvfile = open( \"/Users/digvijaydesai/Downloads/ashita_code/Data.csv\", 'r')\n",
    "    reader = csv.DictReader( csvfile )\n",
    "    #print(reader)\n",
    "    for each in reader:\n",
    "        #print(each)\n",
    "        row={}\n",
    "        blockLoc = {}\n",
    "        for field in header:\n",
    "            row[field]=each[field]\n",
    "            blockLoc[\"path\"] = path+ \"/\"+ each[\"\\ufeffCountry Name\"]\n",
    "            blockLoc[\"type\"] = \"FILE\"\n",
    "        #print (row)\n",
    "        db.df.insert_one(row)\n",
    "        db.blockLocations.insert_one(blockLoc)\n",
    "    return(\"Inserted Data\")\n",
    "\n",
    "#session = db.getMongo().startSession( { readPreference: { mode: \"primary\" } } )\n",
    "\n",
    "\n",
    "######################\n",
    "# Database Functions #\n",
    "######################\n",
    "\n",
    "def delete(list):\n",
    "    try:\n",
    "        for item in list:\n",
    "            col=db[f\"item\"]\n",
    "            col.drop()\n",
    "        return \"Dropped tables\"\n",
    "    except:\n",
    "        return \"Database drop error\"\n",
    "\n",
    "def new_env(edfs):\n",
    "    try:\n",
    "        db = client['edfs']\n",
    "        db.df.insert_one({\"path\":'/root',\"type\":'DIRECTORY'})\n",
    "        return f\"{edfs} created\"\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "\n",
    "def delete_env(edfs):\n",
    "    try:\n",
    "        client.drop_database('edfs')\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} deleted\"\n",
    "\n",
    "def start_env(edfs):\n",
    "    try:\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client['edfs']\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} started\"\n",
    "\n",
    "# def test_edfs(argv):\n",
    "\n",
    "#     edfs = \"edfs\"\n",
    "\n",
    "#     #Testing\n",
    "#     if \"--delete\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#     elif \"--new\" in argv:\n",
    "#         print(new_env(edfs))\n",
    "#     elif \"--restart\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#         print(new_env(edfs))\n",
    "#     else:\n",
    "#         print(start_env(edfs))\n",
    "#         print(mkdir(\"/root\", \"foo\"))\n",
    "#         print(mkdir(\"/root/foo\", \"bar\"))\n",
    "\n",
    "# test_edfs(sys.argv[1])\n",
    "#test_edfs(\"--new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_env(edfs):\n",
    "    env_statements = [\n",
    "            f\"CREATE DATABASE IF NOT EXISTS  {edfs}\",\n",
    "            f\"USE {edfs}\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE df (\n",
    "                    path varchar(255),\n",
    "                    type varchar(255),\n",
    "                    PRIMARY KEY(path)\n",
    "                )\"\"\",\n",
    "            \"INSERT INTO df VALUES ('/root', 'DIRECTORY')\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE blockLocations (\n",
    "                    path varchar(255),\n",
    "                    partition_name varchar(255),\n",
    "                    CONSTRAINT FOREIGN KEY (path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                )\"\"\"\n",
    "    ]\n",
    "    for s in env_statements:\n",
    "        mycursor.execute(s)\n",
    "    mydb.commit()\n",
    "    return f\"{edfs} created\"\n",
    "\n",
    "def seek(path):\n",
    "    seek_statement = \"SELECT * FROM df WHERE path = %s\"\n",
    "    mycursor.execute(seek_statement, (path,))\n",
    "    #myresult = mycursor.fetchall()\n",
    "    myresult={}\n",
    "    for row in mycursor:\n",
    "        myresult= {\"path\":row[0], \"type\":row[1]}\n",
    "    #return myresult\n",
    "    #print(\"seek\", myresult)\n",
    "    if myresult:\n",
    "        #print(\"sdhg\",myresult)\n",
    "        return myresult\n",
    "\n",
    "\n",
    "def mkdir(path, name):\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    #print(\"mkdir\", result)\n",
    "    if result:\n",
    "        if result[\"type\"] == \"DIRECTORY\":\n",
    "            pathname = f\"{path}/{name}\"\n",
    "            dup_result = seek(pathname)\n",
    "            #collection = db[df]\n",
    "            if not dup_result:\n",
    "                insert_statement = \"INSERT INTO df VALUES (%s, 'DIRECTORY')\"\n",
    "                mycursor.execute(insert_statement, (pathname,))\n",
    "                db.df.insert_one(result)\n",
    "                #mydb.commit()\n",
    "                output = f\"directory {name} created\"\n",
    "            else:\n",
    "                output = \"directory already exists\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    print(\"mkdir\",output)\n",
    "\n",
    "def put(path, name, csv):\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[\"type\"] == \"DIRECTORY\":\n",
    "            dup_result = seek(f\"{path}/{name}\")\n",
    "            if not dup_result:\n",
    "                hash_lists = hash(path, name, csv)\n",
    "                output = f\"file {name} created\"\n",
    "            else:\n",
    "                output = \"file already exists\"\n",
    "        else:\n",
    "            output = \"cannot place a file in a file\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    print(\"put\", output)\n",
    "    return output\n",
    "\n",
    "def hash(path, name, csv_file):\n",
    "    #execute metadata alter\n",
    "    doc={\"path\":f\"{path}/{name}\",\"type\":'FILE'}\n",
    "    #meta_statement = \"INSERT INTO df VALUES (%s, 'FILE');\"\n",
    "    #mycursor.execute(meta_statement, (f\"{path}/{name}\",))\n",
    "    #result = mycursor.fetchall()\n",
    "    #mydb.commit()\n",
    "    db.df.insert_one(doc)\n",
    "    with open(csv_file) as f:\n",
    "\n",
    "        key_list, csv_counter = {}, 0\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        header = next(reader)\n",
    "        key_index = key_idx(header)\n",
    "        key = key_cleaning(header)\n",
    "\n",
    "        #TODO: modularize this create code JFC\n",
    "        try:\n",
    "            #create_statement = f\"\"\"\n",
    "             #   CREATE TABLE IF NOT EXISTS {key}(\n",
    "              #      path varchar(255),\n",
    "              #      data_index int,\n",
    "              #      data text,\n",
    "              #      FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "              #  )\"\"\"\n",
    "            #insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "            ##mycursor.execute(create_statement)\n",
    "            #result=mycursor.fetchall()\n",
    "            #mydb.commit()\n",
    "            #mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(header)))\n",
    "            #mydb.commit()\n",
    "            f\"db.getCollection({key})\"\n",
    "            #session.commitTransaction()\n",
    "            row= {\"path\":path+\"/\"+name, \"data_index\":csv_counter,\"data\":','.join(header)}\n",
    "            f\"db.{key}.insert_one(row)\"\n",
    "            #f\"db.{key}.insert_one(path+\"/\"+name,csv)\"\n",
    "            key_list[key] = None\n",
    "            csv_counter += 1\n",
    "        except:\n",
    "            output = f\"ERROR: {mycursor.statement}\"\n",
    "\n",
    "        for row in reader:\n",
    "            key = key_cleaning(row)\n",
    "            #try insert data into datanode\n",
    "            try:\n",
    "                #create_statement = f\"\"\"\n",
    "                #    CREATE TABLE IF NOT EXISTS {key}(\n",
    "                #        path varchar(255),\n",
    "                #        data_index int,\n",
    "                #        data text,\n",
    "                #        FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                #    )\"\"\"\n",
    "                #insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "                #mycursor.execute(create_statement)\n",
    "                #result = mycursor.fetchall()\n",
    "                #mydb.commit()\n",
    "                #mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(row)))\n",
    "                #result = mycursor.fetchall()\n",
    "                #mydb.commit()\n",
    "                f\"db.getCollection({key})\"\n",
    "                #session.commitTransaction()\n",
    "                r= {\"path\":path+\"/\"+name, \"data_index\":csv_counter,\"data\":','.join(row)}\n",
    "                f\"db.{key}.insert_one(r)\"\n",
    "                key_list[key] = None\n",
    "                csv_counter += 1\n",
    "            except:\n",
    "                output = f\"ERROR: {mycursor.statement}\"\n",
    "                rm(path, name)\n",
    "                # return output\n",
    "\n",
    "        #write data into datanodes\n",
    "        for key in key_list.keys():\n",
    "             #block_statement = \"INSERT INTO blockLocations VALUES(%s, %s);\"\n",
    "             #mycursor.execute(block_statement, (f\"{path}/{name}\", key))\n",
    "             row={\"path\":f\"{path}/{name}\",\"type\":key}\n",
    "             db.blockLocations.insert_one(row)\n",
    "        mydb.commit()\n",
    "        return key_list\n",
    "\n",
    "def Sort_Tuple(tup: list, idx: int) -> list:\n",
    "    return(sorted(tup, key = lambda x: x[idx]))\n",
    "\n",
    "def key_idx(str_list):\n",
    "    try:\n",
    "        return str_list.index('Country Name')\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def key_cleaning(row):\n",
    "    #key cleaning\n",
    "    clean_key =  re.sub(r'[^A-Za-z0-9 ]+', '', row[0]).replace(\" \", \"_\")\n",
    "    key = clean_key if clean_key != \"\" else \"invalid_key\"\n",
    "    if key.isnumeric() and key.length() > 0:\n",
    "        key = f\"t{key}\"\n",
    "    if len(key) >= 64:\n",
    "        key = key[0:40]\n",
    "    return key\n",
    "\n",
    "def rm(path, name):\n",
    "    result = seek(path)\n",
    "    filepath =  f\"{path}/{name}\"\n",
    "    if result:\n",
    "        select_statement = \"SELECT * FROM df WHERE path LIKE %s\"\n",
    "        mycursor.execute(select_statement, (filepath + \"%\",))\n",
    "        result = mycursor.fetchall()\n",
    "        row={}\n",
    "        for i in mycursor:\n",
    "            row= {\"path\":i[0], \"type\":i[1]}\n",
    "            db.df.insert_one(row)\n",
    "        if len(result) != 1:\n",
    "            output = \"invalid deletion\"\n",
    "        else:\n",
    "            delete_statement = \"DELETE FROM df WHERE path LIKE %s\"\n",
    "            mycursor.execute(delete_statement, (filepath,)) #TODO: adding % here will add -r functionality\n",
    "            result=mycursor.fetchall()\n",
    "            row={}\n",
    "            for i in mycursor:\n",
    "                row= {\"path\":i[0], \"type\":i[1]}\n",
    "                db.df.insert_one(row)\n",
    "            if(row):\n",
    "                print(row)\n",
    "            mydb.commit()\n",
    "            output = f\"{filepath} deleted\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {filepath}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def ls(path):\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[\"type\"] == \"FILE\":\n",
    "            output = \"Cannot run 'ls' on files\"\n",
    "        elif result[\"type\"] == \"DIRECTORY\":\n",
    "            ls_statement = \"SELECT * FROM df WHERE path REGEXP %s\"\n",
    "            mycursor.execute(ls_statement, (f\"^{path}\\/[^\\/]+$\",))\n",
    "            output = mycursor.fetchall()\n",
    "            row={}\n",
    "            for i in output:\n",
    "                row= {\"path\":i[0], \"type\":i[1]}\n",
    "                db.df.insert_one(row)\n",
    "            output1=db.df.find()\n",
    "    else:\n",
    "        output1 = f\"Invalid path: {path}\"\n",
    "    output2 = []\n",
    "    for o in output1:\n",
    "        output2.append(o['path'])\n",
    "    return ', '.join(output2)\n",
    "\n",
    "\n",
    "def getPartitionLocations(path):\n",
    "    cat_statement = \"SELECT * FROM blockLocations WHERE path = %s\"\n",
    "    mycursor.execute(cat_statement, (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    row={}\n",
    "    for i in result:\n",
    "        row= {\"path\":i[0], \"type\":i[1]}\n",
    "        db.df.insert_one(row)\n",
    "    x=db.blockLocations.find()\n",
    "    for j in x:\n",
    "        print(\"getPartitionLocations(path):\", j)\n",
    "    return x\n",
    "\n",
    "def getPartitionData(path):\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    if result:\n",
    "        if result[\"type\"] == \"DIRECTORY\":\n",
    "            output = \"Cannot run 'cat' on directories\"\n",
    "        elif result[\"type\"] == \"FILE\":\n",
    "            myresult = getPartitionLocations(path)\n",
    "            data_list = []\n",
    "            for partition in myresult:\n",
    "                data_list = data_list + readPartition(path, partition[1])\n",
    "            sorted_data_list = Sort_Tuple(data_list, 1)\n",
    "            return sorted_data_list\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    print(\"getPartitionData(path):\", output)\n",
    "    return output\n",
    "\n",
    "def cat(path):\n",
    "    output = \"\"\n",
    "    sorted_data_list = getPartitionData(path)\n",
    "    for s in sorted_data_list:\n",
    "        output += s[2] +\"\\n\"\n",
    "    print(\"Cat\", output)\n",
    "    return output\n",
    "\n",
    "def getPartitionLocations(path):\n",
    "    cat_statement = \"SELECT * FROM blockLocations WHERE path = %s\"\n",
    "    mycursor.execute(cat_statement, (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    row={}\n",
    "    for i in result:\n",
    "        row= {\"path\":i[0], \"type\":i[1]}\n",
    "        db.df.insert_one(row)\n",
    "    x=db.blockLocations.find()\n",
    "    # for j in x:\n",
    "    #     print(\"getPartitionLocations(path):\", j)\n",
    "    # return x\n",
    "    output = []\n",
    "    for o in x:\n",
    "        output.append(str(o))\n",
    "    return ', '.join(output)\n",
    "\n",
    "def readPartition(path, partition_name):\n",
    "    mycursor.execute(f\"SELECT * FROM {partition_name} WHERE path = %s\", (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    partition_data = []\n",
    "    for line in result:\n",
    "        partition_data.append((partition_name, line[1], line[2]))\n",
    "    print(\"readPartition\", partition_data)\n",
    "    return partition_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "put(\"/root/foo\", \"data\", \"datasets/Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = getPartitionLocations(\"/root/foo/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "1146 (42S02): Table 'edfs_mongo.Andorra' doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 90\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y203sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m readPartition(\u001b[39m\"\u001b[39;49m\u001b[39m/root/foo/data\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mAndorra\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 90\u001b[0m in \u001b[0;36mreadPartition\u001b[0;34m(path, partition_name)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y203sZmlsZQ%3D%3D?line=281'>282</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadPartition\u001b[39m(path, partition_name):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y203sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m     mycursor\u001b[39m.\u001b[39;49mexecute(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSELECT * FROM \u001b[39;49m\u001b[39m{\u001b[39;49;00mpartition_name\u001b[39m}\u001b[39;49;00m\u001b[39m WHERE path = %s\u001b[39;49m\u001b[39m\"\u001b[39;49m, (path,))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y203sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m     result \u001b[39m=\u001b[39m mycursor\u001b[39m.\u001b[39mfetchall()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y203sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m     partition_data \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/mysql/connector/cursor.py:564\u001b[0m, in \u001b[0;36mMySQLCursor.execute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_iter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mcmd_query_iter(stmt))\n\u001b[1;32m    563\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mcmd_query(stmt))\n\u001b[1;32m    565\u001b[0m \u001b[39mexcept\u001b[39;00m InterfaceError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    566\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mhave_next_result:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/mysql/connector/connection.py:990\u001b[0m, in \u001b[0;36mMySQLConnection.cmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    988\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(packet)\n\u001b[1;32m    989\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_cmd(ServerCmd\u001b[39m.\u001b[39;49mQUERY, query))\n\u001b[1;32m    991\u001b[0m \u001b[39mexcept\u001b[39;00m ProgrammingError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    992\u001b[0m     \u001b[39mif\u001b[39;00m err\u001b[39m.\u001b[39merrno \u001b[39m==\u001b[39m \u001b[39m3948\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mLoading local data is disabled\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m err\u001b[39m.\u001b[39mmsg:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/mysql/connector/connection.py:784\u001b[0m, in \u001b[0;36mMySQLConnection._handle_result\u001b[0;34m(self, packet)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_eof(packet)\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m packet[\u001b[39m4\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m255\u001b[39m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[39mraise\u001b[39;00m get_exception(packet)\n\u001b[1;32m    786\u001b[0m \u001b[39m# We have a text result set\u001b[39;00m\n\u001b[1;32m    787\u001b[0m column_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_protocol\u001b[39m.\u001b[39mparse_column_count(packet)\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 1146 (42S02): Table 'edfs_mongo.Andorra' doesn't exist"
     ]
    }
   ],
   "source": [
    "readPartition(\"/root/foo/data\", \"Andorra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getPartitionData(path): Invalid path: /root/foo/data\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 89\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cat(\u001b[39m\"\u001b[39;49m\u001b[39m/root/foo/data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 89\u001b[0m in \u001b[0;36mcat\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m sorted_data_list \u001b[39m=\u001b[39m getPartitionData(path)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sorted_data_list:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m     output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m s[\u001b[39m2\u001b[39;49m] \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=262'>263</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCat\u001b[39m\u001b[39m\"\u001b[39m, output)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y201sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "cat(\"/root/foo/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ls(\"/root/foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root, /root/foo, /root/foo/data, /root/foo/data, /root/foo, /root/foo, /root/foo/bar, /root/foo, /root/foo/bar, /root/foo/bar, /root/foo/bar, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo, /root/foo/bar, /root/foo/bar'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir directory bar created\n"
     ]
    }
   ],
   "source": [
    "mkdir(\"/root/foo\", \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "No result set to fetch from",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 93\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rm(\u001b[39m\"\u001b[39;49m\u001b[39m/root/foo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb Cell 93\u001b[0m in \u001b[0;36mrm\u001b[0;34m(path, name)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m delete_statement \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDELETE FROM df WHERE path LIKE \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m mycursor\u001b[39m.\u001b[39mexecute(delete_statement, (filepath,)) \u001b[39m#TODO: adding % here will add -r functionality\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m result\u001b[39m=\u001b[39mmycursor\u001b[39m.\u001b[39;49mfetchall()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m row\u001b[39m=\u001b[39m{}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/tools.ipynb#Y161sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m mycursor:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/mysql/connector/cursor.py:1035\u001b[0m, in \u001b[0;36mMySQLCursorBuffered.fetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39m\"\"\"Return all rows of a query result set.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \n\u001b[1;32m   1031\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m    list: A list of tuples with all rows of a query result set.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[39mraise\u001b[39;00m InterfaceError(ERR_NO_RESULT_TO_FETCH)\n\u001b[1;32m   1036\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m   1037\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_row :]\n",
      "\u001b[0;31mInterfaceError\u001b[0m: No result set to fetch from"
     ]
    }
   ],
   "source": [
    "rm(\"/root/foo\", \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir directory bar created\n"
     ]
    }
   ],
   "source": [
    "mkdir(\"/root/foo\", \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as ccnx\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "########################\n",
    "# mongodb client setup #\n",
    "########################\n",
    "\n",
    "client = MongoClient('mongodb://root:root@localhost:27017/')\n",
    "db = client['edfs']\n",
    "# db = new Mongo().getDB(\"edfs\")\n",
    "#collection = db['name_of_collection']\n",
    "\n",
    "#python connector setup\n",
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs_mongo\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(1, '../edfs')\n",
    "import edfs.mysql as sql\n",
    "import edfs.firebase as firebase\n",
    "import mysql.connector as ccnx\n",
    "from enum import Enum\n",
    "\n",
    "# class syntax\n",
    "class EDFS(Enum):\n",
    "    MYSQL = 1\n",
    "    FIREBASE = 2\n",
    "    MONGODB = 3\n",
    "\n",
    "class FUNC(Enum):\n",
    "    SUM = 1\n",
    "    MAX = 2\n",
    "    MIN = 3\n",
    "    AVG = 4\n",
    "\n",
    "\n",
    "#####################\n",
    "#  Helper Function  #\n",
    "#####################\n",
    "\n",
    "def convert_str(item:str):\n",
    "    try:\n",
    "        return float(item)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#####################\n",
    "#  Mapper Function  #\n",
    "#####################\n",
    "\n",
    "def mapPartition(key:str, col_data, data:str):\n",
    "    \"\"\"\n",
    "        Arg: The name of the partition from getPartitionLocations\n",
    "        col_data: the names of the columns\n",
    "        data: the contents of the data itself\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "def firebase_map(targets, file):\n",
    "    data = firebase.cat(file)\n",
    "    data = [row.split(\",\") for row in data] #list of list\n",
    "    header_list = data[0]\n",
    "    data = [(row[0], None, ','.join(row)) for row in data]\n",
    "    col_dict = {header_list.index(i):i for i in targets}\n",
    "    return edfs_map(data, col_dict)\n",
    "\n",
    "def edfs_map(data, col_dict):\n",
    "    #map step\n",
    "    data_mapped = {}\n",
    "\n",
    "    for d in data:\n",
    "        partition_key, data_block = d[0], d[2].split(\",\")\n",
    "        data_block = [(col_dict[x], data_block[x]) for x in col_dict.keys()]\n",
    "        if partition_key not in data_mapped:\n",
    "            data_mapped[partition_key] = []\n",
    "        data_mapped[partition_key] += data_block\n",
    "    return data_mapped\n",
    "\n",
    "def sql_map(targets: [], file:str):\n",
    "    sql.start_env(\"edfs\")\n",
    "    data = sql.getPartitionData(file)\n",
    "    # then I get all the partition locations and the indices and it goes zoooom\n",
    "\n",
    "    #ID targets\n",
    "    header_list = (data[0][2].split(\",\"))\n",
    "    col_dict = {header_list.index(i):i for i in targets}\n",
    "    return edfs_map(data, col_dict)\n",
    "\n",
    "def edfs_shuffle(data_mapped:dict):\n",
    "    data_shuffled = {}\n",
    "    for key in data_mapped.keys():\n",
    "        for item in data_mapped[key]:\n",
    "            col_key, value = item[0], item[1]\n",
    "            if col_key not in data_shuffled:\n",
    "                data_shuffled[col_key] = []\n",
    "            data_shuffled[col_key].append(convert_str(value))\n",
    "    return data_shuffled\n",
    "\n",
    "def edfs_reduce(data_shuffled:dict, function:int):\n",
    "    data_reduced = {}\n",
    "    if function == FUNC.SUM:\n",
    "        for col in data_shuffled.keys():\n",
    "            data_reduced[col] = sum(list(filter(lambda item: item is not None, data_shuffled[col])))\n",
    "    elif function == FUNC.MAX:\n",
    "        for col in data_shuffled.keys():\n",
    "            data_reduced[col] = max(list(filter(lambda item: item is not None, data_shuffled[col])))\n",
    "    elif function == FUNC.MIN:\n",
    "        for col in data_shuffled.keys():\n",
    "            data_reduced[col] = min(list(filter(lambda item: item is not None, data_shuffled[col])))\n",
    "    elif function == FUNC.AVG:\n",
    "        for col in data_shuffled.keys():\n",
    "            shuffled_data = list(filter(lambda item: item is not None, data_shuffled[col]))\n",
    "            data_reduced[col] = sum(shuffled_data)/len(shuffled_data)\n",
    "    return data_reduced\n",
    "\n",
    "def execute(implementation:int, function:int, targets:[]=None, file:str=None, DEBUG=False):\n",
    "    #TODO import getPartitionLocations() from each\n",
    "    if implementation == EDFS.MYSQL:\n",
    "        data_mapped = sql_map(targets, file)\n",
    "        data_shuffled = edfs_shuffle(data_mapped)\n",
    "        data_reduced = edfs_reduce(data_shuffled, function)\n",
    "        if DEBUG:\n",
    "            print(f\"Data Mapped:\\n {data_mapped}\\n\")\n",
    "            print(f\"Data Shuffled:\\n {data_shuffled}\\n\")\n",
    "            print(f\"Data Reduced:\\n {data_reduced}\\n\")\n",
    "        return data_reduced\n",
    "    if implementation == EDFS.FIREBASE:\n",
    "        data_mapped = firebase_map(targets, file)\n",
    "        data_shuffled = edfs_shuffle(data_mapped)\n",
    "        data_reduced = edfs_reduce(data_shuffled, function)\n",
    "        if DEBUG:\n",
    "            print(f\"Data Mapped:\\n {data_mapped}\\n\")\n",
    "            print(f\"Data Shuffled:\\n {data_shuffled}\\n\")\n",
    "            print(f\"Data Reduced:\\n {data_reduced}\\n\")\n",
    "        return data_reduced\n",
    "        return data_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python connector setup\n",
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs_mongo\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = dict((key[:4], value) for (key, value) in reduced_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2019': 10.214665351311616, '2020': 9.971583415248919}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Year': list(reduced_data.keys()), 'Value': list(reduced_data.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keys</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>10.214665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>9.971583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Keys     Values\n",
       "0  2019  10.214665\n",
       "1  2020   9.971583"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_data = execute(mycursor, EDFS.FIREBASE, FUNC.AVG, targets=[\"2015 [YR2015]\", \"2020 [YR2020]\"], file=\"root/user/Stats_Cap_Ind_Sample\", DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = [\"2000\", \"2017\"]\n",
    "# for i in range (data[0], data[1])\n",
    "YEAR_RANGE = [f\"{year} [YR{year}]\" for year in range(int(date_range[0]), int(date_range[1]))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = execute(\n",
    "#     EDFS.FIREBASE,  FUNC.MAX,\n",
    "#     ['2000 [YR2000]', '2001 [YR2001]', '2002 [YR2002]', '2003 [YR2003]', '2004 [YR2004]', '2005 [YR2005]', '2006 [YR2006]', '2007 [YR2007]', '2008 [YR2008]', '2009 [YR2009]', '2010 [YR2010]', '2011 [YR2011]', '2012 [YR2012]', '2013 [YR2013]', '2014 [YR2014]', '2015 [YR2015]', '2016 [YR2016]', '2017 [YR2017]', '2018 [YR2018]', '2019 [YR2019]'],\n",
    "# f\"root/user/Stats_Cap_Ind_Sample\", DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(mycursor, EDFS.FIREBASE, FUNC.AVG, targets=[\"2015 [YR2015]\", \"2020 [YR2020]\"], file=\"root/user/Stats_Cap_Ind_Sample\", DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_RANGE = ['2002 [YR2002]', '2003 [YR2003]', '2004 [YR2004]', '2005 [YR2005]', '2006 [YR2006]', '2007 [YR2007]', '2008 [YR2008]', '2009 [YR2009]', '2010 [YR2010]', '2011 [YR2011]', '2012 [YR2012]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2002 [YR2002]': 100.0,\n",
       " '2003 [YR2003]': 100.0,\n",
       " '2004 [YR2004]': 100.0,\n",
       " '2005 [YR2005]': 100.0,\n",
       " '2006 [YR2006]': 100.0,\n",
       " '2007 [YR2007]': 100.0,\n",
       " '2008 [YR2008]': 100.0,\n",
       " '2009 [YR2009]': 100.0,\n",
       " '2010 [YR2010]': 100.0,\n",
       " '2011 [YR2011]': 100.0,\n",
       " '2012 [YR2012]': 100.0}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = execute(\n",
    "    EDFS.MYSQL, FUNC.MAX,\n",
    "    targets=YEAR_RANGE, file=\"/root/foo/data\", DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019 [YR2019]', '2020 [YR2020]']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{year} [YR{year}]\" for year in [\"2019\", \"2020\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if sys.argv[2] == \"mysql\":\n",
    "        #python connector setup\n",
    "        mydb = ccnx.connect(\n",
    "          host=\"localhost\",\n",
    "          user=\"root\",\n",
    "          password=sys.argv[1],\n",
    "        )\n",
    "        mycursor = mydb.cursor(buffered=True)\n",
    "        execute(mycursor, EDFS.MYSQL, FUNC.MIN, targets=[\"2019 [YR2019]\", \"2020 [YR2020]\"], file=\"/root/foo/data\", DEBUG=True)\n",
    "        execute(mycursor, EDFS.FIREBASE, FUNC.AVG, targets=[\"2019 [YR2019]\", \"2020 [YR2020]\"], file=\"root/user/Stats_Cap_Ind_Sample\", DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edfs.pmr as pmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edfs.pmr' from '/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/edfs/pmr.py'>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = pmr.execute(\n",
    "    \"MYSQL\", \"MIN\",\n",
    "    targets=YEAR_RANGE, file=\"/root/foo/data\", DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002</td>\n",
       "      <td>1.253706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2003</td>\n",
       "      <td>2.591462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005</td>\n",
       "      <td>3.207317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006</td>\n",
       "      <td>0.643132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007</td>\n",
       "      <td>1.031562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>1.279290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>2.684685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012</td>\n",
       "      <td>3.136361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year     Value\n",
       "0   2002  1.253706\n",
       "6   2003  2.591462\n",
       "9   2004  3.500000\n",
       "3   2005  3.207317\n",
       "5   2006  0.643132\n",
       "2   2007  1.031562\n",
       "7   2008  1.279290\n",
       "4   2009  1.900000\n",
       "10  2010  1.500000\n",
       "1   2011  2.684685\n",
       "8   2012  3.136361"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data.sort_values(by='Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EDFS.MYSQL: 1>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDFS.MYSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "\n",
    "firebase_url = 'https://dsci551-project-52d43-default-rtdb.firebaseio.com/'\n",
    "\n",
    "def seek(path):\n",
    "    url = firebase_url + path + '.json'\n",
    "    try:\n",
    "        rget = requests.get(url)\n",
    "        return rget\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        \n",
    "\n",
    "def ls(path: str) -> str:\n",
    "    '''List files under path/.\n",
    "\n",
    "    Args:\n",
    "        path (str): path starting from NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    slist = seek(f\"NameNode/{path}\")\n",
    "    rlist = slist.json()\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    if type(rlist) == dict: # iterate over rlist if rlist != None\n",
    "        for key, value in rlist.items():\n",
    "            if key == \"_\": # empty directory\n",
    "                continue\n",
    "            if type(value) == dict: # if item is folder, add '/' to the end of thge string\n",
    "                result_list.append(key + \"/\")\n",
    "            else:\n",
    "                result_list.append(key)\n",
    "        output = 'empty' if not result_list else ', '.join(result_list)\n",
    "    elif not rlist:\n",
    "        output = f'Path {path} not found'\n",
    "    else: \n",
    "        output = f'{path} is not a folder'\n",
    "    \n",
    "    return output\n",
    "\n",
    "        \n",
    "def mkdir(path: str) -> str:\n",
    "    '''Create directory if not exists\n",
    "\n",
    "    Args:\n",
    "        path: relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    full_path = f'NameNode/{path}'\n",
    "    if seek(full_path).json() is None:\n",
    "        url = firebase_url + full_path + '.json'\n",
    "        data = '{\"_\" : \"_\"}' # empty directory\n",
    "        r = requests.put(url,data)\n",
    "        output = f'Directory {path} created'\n",
    "    else:\n",
    "        output  = 'Directory ' + path + ' already exists'\n",
    "    return output\n",
    "\n",
    "        \n",
    "def rm(path: str) -> str:\n",
    "    '''Delete directory if exists\n",
    "\n",
    "    Args:\n",
    "        path: relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    full_path = f'NameNode/{path}'\n",
    "    if seek(full_path).json() is None:\n",
    "        output = 'Directory not found'\n",
    "    else:\n",
    "        url = firebase_url + full_path + '.json'\n",
    "        d = requests.delete(url)\n",
    "        if d.status_code == 200:\n",
    "            output = path + ' was succefully deleted'\n",
    "    return output\n",
    "\n",
    "\n",
    "def getPartitionLocation(file: str) -> str:\n",
    "    '''Return the locations of partitions of the\n",
    "        file\n",
    "    Args:\n",
    "        file (str): relative path to NameNode/\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    path = \"NameNode/\" + file + \"/partitions\"\n",
    "    rpath = seek(path)\n",
    "    partition = requests.get(rpath.url)\n",
    "    pdict = partition.json()       \n",
    "\n",
    "    if pdict is None:\n",
    "        output = f'Partitions for {file} not found'\n",
    "    else:\n",
    "        output = json.dumps(pdict, indent=4, sort_keys=True) # Organizing the data\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def readPartition(file, partition) -> str:\n",
    "    '''Return the content of partition # of\n",
    "    the specified file\n",
    "    Args:\n",
    "        file (str): relative path to NameNode/\n",
    "        partition (str):  name of the partition\n",
    "    Returns:\n",
    "        (str) Success or Error message\n",
    "    '''\n",
    "    try:\n",
    "        pdict = json.loads(getPartitionLocation(file))\n",
    "        url = pdict[partition]\n",
    "        pdict = requests.get(url).json()\n",
    "        output = json.dumps(pdict, indent=4, sort_keys=True)\n",
    "    except:\n",
    "        output = 'Partition not found'\n",
    "    return output\n",
    "\n",
    "\n",
    "# cleans column names for firebase json object key\n",
    "def varname (var):\n",
    "    key = re.sub(r'[^A-Za-z0-9 ]+', '', var).replace(\" \", \"_\")\n",
    "    names = key if key != \"\" else \"invalid_key\"\n",
    "    return names\n",
    "\n",
    "def mtime():\n",
    "#     to revert back\n",
    "    #datetime.datetime.utcfromtimestamp(int(mtime)/1000).strftime('%Y-%-m-%-d %I:%M:%S') \n",
    "    return (datetime.datetime.now().timestamp()*1000)\n",
    "\n",
    "def filesize(file): #file size in bytes\n",
    "    return  os.path.getsize(file)\n",
    "\n",
    "def indexing(dicts):\n",
    "    dt = dict()\n",
    "    for k,v in dicts.items():\n",
    "        i = int(k.replace('p',''))\n",
    "        dt[i] = v\n",
    "    return dt\n",
    "\n",
    "def record_partition(path, country, filename, url):\n",
    "    try:\n",
    "        npath = firebase_url + path + \"/\" + filename + \"/partitions.json\"\n",
    "    #     print (npath ,\":\", url)\n",
    "        mdata = {country : url}\n",
    "        putMeta = requests.patch(npath, json.dumps(mdata))\n",
    "        if putMeta.status_code == 400: print(country)\n",
    "    #     print (putMeta)\n",
    "    except:\n",
    "        print (country)\n",
    "\n",
    "def file_mdata(path, file, filename):\n",
    "    npath = firebase_url + path + \"/\" + filename + \".json\"\n",
    "    mdata = {'ctime': mtime(),\n",
    "             'name': file,\n",
    "             'type': 'FILE',\n",
    "             'filesize':filesize(file)}\n",
    "    putMeta = requests.patch(npath, json.dumps(mdata))\n",
    "    \n",
    "\n",
    "# partition by Country (Original plan)\n",
    "def put(file: str, path: str) -> str:\n",
    "    filename = file.replace(\".csv\",\"\")\n",
    "    path = 'NameNode/' + path\n",
    "\n",
    " \n",
    "    # creating dictinary to organize data into correct json format. \n",
    "    # added 'file name' to the dictionary to help differentiate data from different files\n",
    "    dc = dict()\n",
    "    with open(file, encoding = 'utf-8') as csvfile:\n",
    "        csvReader = csv.reader(csvfile)\n",
    "        \n",
    "        for index, row in enumerate(csvReader):\n",
    "            cname = varname(row[0])\n",
    "            n = 'p' + str(index)\n",
    "            if cname in dc:\n",
    "                dc[cname][n] = (';'.join(row))\n",
    "            else:\n",
    "                dc[cname]={n:(';'.join(row))}\n",
    "    \n",
    "    if seek(path + '/' +filename).json() is None:\n",
    "        for key, val in dc.items():\n",
    "            url = firebase_url + 'DataNode/' + key + '/' + filename + '.json'\n",
    "            putResponse = requests.put(url, json.dumps(val))\n",
    "            if putResponse.status_code == 200:\n",
    "                record_partition (path, key, filename, putResponse.url)\n",
    "            else:\n",
    "                print (file, 'failed to uploaded at partition', key)\n",
    "        \n",
    "        output =  file + ' was succesfully uploaded to ' + path\n",
    "        \n",
    "        file_mdata(path, file, filename)\n",
    "        #add metadata information.\n",
    "    else:\n",
    "        output = file + \" already exists in \" + path\n",
    "            \n",
    "        \n",
    "    return output\n",
    "    \n",
    "\n",
    "def cat(path):\n",
    "    file = path.replace('.csv','')\n",
    "    pdict = json.loads(getPartitionLocation(file))\n",
    "    data = dict()\n",
    "    for k,v in pdict.items():\n",
    "        getPartition = requests.get(v).json()\n",
    "        for key, val in getPartition.items():\n",
    "            i = int(key.replace('p',''))\n",
    "            data[i]=val.replace(';',',')\n",
    "            \n",
    "    ldata = list()\n",
    "    for key in sorted(data):\n",
    "        ldata.append(data[key])\n",
    "    return ldata\n",
    "\n",
    "\n",
    "def mapPartition(p, file):\n",
    "    file_name = file.split('/')[-1]\n",
    "    columns = f'https://dsci551-project-52d43-default-rtdb.firebaseio.com/DataNode/Country_Name/{file_name}.json'\n",
    "    rlist =[ v for k, v in requests.get(columns).json().items()]\n",
    "    readMap = indexing(requests.get(p).json())\n",
    "    for key in sorted(readMap):\n",
    "        rlist.append(readMap[key])\n",
    "    return rlist \n",
    "    \n",
    "\n",
    "# function to get year columns\n",
    "def is_year (c):\n",
    "    return any(char.isdigit() for char in c)    \n",
    "\n",
    "\n",
    "def new_col(cols):\n",
    "    new_col = list()\n",
    "    for c in cols:\n",
    "        if is_year(c):\n",
    "            new_col.append(c[:4])\n",
    "        else:\n",
    "            new_col.append(c)\n",
    "    return new_col\n",
    "\n",
    "\n",
    "def to_df(data):\n",
    "    df = pd.DataFrame(columns = data[0].split(';'), data=[row.split(';') for row in data[1:]])\n",
    "    columns = new_col(df.columns.values)\n",
    "    df.columns = columns\n",
    "    df_melted = df.melt(id_vars=columns[:4], var_name='Year', value_name='Value')\n",
    "    return df_melted\n",
    "\n",
    "\n",
    "# function to get year columns\n",
    "def is_year (c):\n",
    "    return any(char.isdigit() for char in c)\n",
    "\n",
    "def read_dataset(file: str):\n",
    "    partitions = json.loads(getPartitionLocation(file))\n",
    "\n",
    "    df_list = list()\n",
    "    for country_name, dir in partitions.items():\n",
    "        if country_name == 'Country_Name': # Store only column names. Ignore\n",
    "            continue\n",
    "        map = mapPartition(dir, file)\n",
    "        df_list.append(to_df(map))\n",
    "        break ### REMOVEE\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    # change columns names\n",
    "    new_columns = list()\n",
    "    columns = df.columns\n",
    "    for c in columns:\n",
    "        if is_year(c):\n",
    "            new_columns.append(c[:4])\n",
    "        else:\n",
    "            new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "    # change column names in dataframe\n",
    "    df.columns = new_columns\n",
    "    df['Year'] = df['Year'].astype(int)\n",
    "    df = df.loc[df['Value'] != '..'].copy()\n",
    "    df['Value'] = df['Value'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def test():\n",
    "    return 'test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Data_Extract_From_Statistical_Capacity_Indicators/42377300-c075-4554-a55f-41cd64c79126_Data.csv\")\n",
    "# df = pd.read_csv(\"datasets/Human_Capital_Index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df['Series Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[df['Series Name'] == 'Per capita GDP growth'].sample(50, random_state=42).copy()\n",
    "df2 = df[df['Series Name'] == 'Consumer price index base year'].sample(50, random_state=42).copy()\n",
    "# Consumer price index base year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv('datasets/Per_Capita_GDP.csv', index=False)\n",
    "df2.to_csv('datasets/Consume_Price_Index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Access to water',\n",
       " 'Agricultural census',\n",
       " 'Balance of payments manual in use',\n",
       " 'Child malnutrition',\n",
       " 'Child mortality',\n",
       " 'Consumer price index base year',\n",
       " 'External debt reporting status',\n",
       " 'Gender equality',\n",
       " 'Government finance accounting',\n",
       " 'HIV/AIDS',\n",
       " 'Health survey',\n",
       " 'Immunization',\n",
       " 'Import and export price indexes',\n",
       " 'Income poverty',\n",
       " 'Industrial production index',\n",
       " 'Maternal health',\n",
       " 'Methodology assessment of statistical capacity (scale 0 - 100)',\n",
       " 'National accounts base year',\n",
       " 'National immunization coverage',\n",
       " 'Overall Average',\n",
       " 'Per capita GDP growth',\n",
       " 'Periodicity and timeliness assessment of statistical capacity (scale 0 - 100)',\n",
       " 'Population census',\n",
       " 'Poverty survey',\n",
       " 'Primary completion',\n",
       " 'Source data assessment of statistical capacity (scale 0 - 100)',\n",
       " 'Special Data Dissemination Standard',\n",
       " 'UNESCO reporting',\n",
       " 'Vital registration system coverage'}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['Series Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed29fcc266c631214eb0a32dcb461d51a7279cf73c87ecfbb65b364d13f4dbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
