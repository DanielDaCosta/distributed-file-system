{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as ccnx\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#python connector setup\n",
    "mydb = ccnx.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"edfs\"\n",
    ")\n",
    "mycursor = mydb.cursor(buffered=True)\n",
    "\n",
    "####################\n",
    "# Helper Functions #\n",
    "####################\n",
    "\n",
    "def Sort_Tuple(tup: list, idx: int) -> list:\n",
    "    '''\n",
    "    Return the sorted list of tuples by the element of the tuple\n",
    "    Args:\n",
    "        tup (list): unsorted list of tuples\n",
    "        idx (int): the element of the tuple to sort by\n",
    "    Returns:\n",
    "        (list) Sorted list of tuples\n",
    "    '''\n",
    "    return(sorted(tup, key = lambda x: x[idx]))\n",
    "\n",
    "def key_idx(str_list):\n",
    "    '''\n",
    "    Return the index of 'Country Name' if it exists in the dataset, and 0 if\n",
    "    the column name is not present\n",
    "    Args:\n",
    "        str_list (list): the list of column names\n",
    "    Returns:\n",
    "        (int) the index returned\n",
    "    '''\n",
    "    try:\n",
    "        return str_list.index('Country Name')\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def key_cleaning(row):\n",
    "    #key cleaning\n",
    "    clean_key =  re.sub(r'[^A-Za-z0-9 ]+', '', row[0]).replace(\" \", \"_\")\n",
    "    key = clean_key if clean_key != \"\" else \"invalid_key\"\n",
    "    if key.isnumeric() and key.length() > 0:\n",
    "        key = f\"t{key}\"\n",
    "    if len(key) >= 64:\n",
    "        key = key[0:40]\n",
    "    return key\n",
    "\n",
    "####################\n",
    "# API Functions    #\n",
    "####################\n",
    "\n",
    "def read_dataset(path):\n",
    "    # (partition_name, csv_index, comma-separated-string)\n",
    "    list_of_tuples = getPartitionData(path)\n",
    "    list_of_lists = [tuple[2].split(\",\") for tuple in list_of_tuples]\n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header #set the header row as the df header\n",
    "\n",
    "    df = df.drop([\"Country Code\", \"Series Code\"], \"columns\")\n",
    "\n",
    "    df_melted = df.melt(id_vars=[\"Country Name\", \"Series Name\"],\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\")\n",
    "\n",
    "    df_melted[\"Year\"] = df_melted[\"Year\"].str[0:4]\n",
    "\n",
    "    df_melted = df_melted.loc[df_melted.Value.str.isnumeric()].copy()\n",
    "\n",
    "    # change columns names\n",
    "    new_columns = list()\n",
    "    columns = df_melted.columns\n",
    "    for c in columns:\n",
    "            new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "    # change column names in dataframe\n",
    "    df_melted.columns = new_columns\n",
    "\n",
    "    return df_melted.astype({'Year':'int', 'Value': 'float'})\n",
    "\n",
    "def seek(path):\n",
    "    '''\n",
    "    Returns the filestructure that matches the specified path\n",
    "    Args:\n",
    "        str_list (list): the list of column names\n",
    "    Returns:\n",
    "        (int) the index returned\n",
    "    '''\n",
    "    seek_statement = \"SELECT * FROM df WHERE path = %s\"\n",
    "    mycursor.execute(seek_statement, (path,))\n",
    "    myresult = mycursor.fetchall()\n",
    "    return myresult\n",
    "\n",
    "def mkdir(path, name):\n",
    "    '''\n",
    "    Create the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            pathname = f\"{path}/{name}\"\n",
    "            dup_result = seek(pathname)\n",
    "            if not dup_result:\n",
    "                insert_statement = \"INSERT INTO df VALUES (%s, 'DIRECTORY')\"\n",
    "                mycursor.execute(insert_statement, (pathname,))\n",
    "                mydb.commit()\n",
    "                output = f\"directory {name} created\"\n",
    "            else:\n",
    "                output = \"directory already exists\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def rm(path, name):\n",
    "    '''\n",
    "    Removes the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    filepath =  f\"{path}/{name}\"\n",
    "    if result:\n",
    "        select_statement = \"SELECT * FROM df WHERE path LIKE %s\"\n",
    "        mycursor.execute(select_statement, (filepath + \"%\",))\n",
    "        result = mycursor.fetchall()\n",
    "        if len(result) != 1:\n",
    "            output = \"invalid deletion\"\n",
    "        else:\n",
    "            delete_statement = \"DELETE FROM df WHERE path LIKE %s\"\n",
    "            mycursor.execute(delete_statement, (filepath,)) #TODO: adding % here will add -r functionality\n",
    "            mydb.commit()\n",
    "            output = f\"{filepath} deleted\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {filepath}\"\n",
    "    return output\n",
    "\n",
    "def ls(path):\n",
    "    '''\n",
    "    Returns the contents of the directory at the specfied path in the filesystem\n",
    "    Args:\n",
    "        path (str): the path to the directory's home\n",
    "        name (str): the name of the directory\n",
    "    Returns:\n",
    "        output (str): success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"FILE\":\n",
    "            output = \"Cannot run 'ls' on files\"\n",
    "        elif result[0][1] == \"DIRECTORY\":\n",
    "            ls_statement = \"SELECT * FROM df WHERE path REGEXP %s\"\n",
    "            mycursor.execute(ls_statement, (f\"^{path}\\/[^\\/]+$\",))\n",
    "            output = mycursor.fetchall()\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def getPartitionLocations(path):\n",
    "    '''\n",
    "    Returns the blockLocations that match the file at the specified\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the blockLocations in a list of tuples\n",
    "    '''\n",
    "    cat_statement = \"SELECT * FROM blockLocations WHERE path = %s\"\n",
    "    mycursor.execute(cat_statement, (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    return result\n",
    "\n",
    "def readPartition(path, partition_name):\n",
    "    '''\n",
    "    Returns the contents of a specified partition_name\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        partition_name: the name of the partition\n",
    "    Returns:\n",
    "        [list of (tuple)]: the data contents of the partition\n",
    "    '''\n",
    "    mycursor.execute(f\"SELECT * FROM {partition_name} WHERE path = %s\", (path,))\n",
    "    result = mycursor.fetchall()\n",
    "    partition_data = []\n",
    "    for line in result:\n",
    "        partition_data.append((partition_name, line[1], line[2]))\n",
    "    return partition_data\n",
    "\n",
    "def cat(path):\n",
    "    '''\n",
    "    Returns the contents the file at the specified path\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the text of the file\n",
    "    '''\n",
    "    output = \"\"\n",
    "    sorted_data_list = getPartitionData(path)\n",
    "    for s in sorted_data_list:\n",
    "        output += s[2] +\"\\n\"\n",
    "    return output\n",
    "\n",
    "def getPartitionData(path):\n",
    "    '''\n",
    "    Returns the contents the file at the specified path\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "    Returns:\n",
    "        output (obj): the text of the file\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            output = \"Cannot run 'cat' on directories\"\n",
    "        elif result[0][1] == \"FILE\":\n",
    "            myresult = getPartitionLocations(path)\n",
    "            data_list = []\n",
    "            for partition in myresult:\n",
    "                data_list = data_list + readPartition(path, partition[1])\n",
    "            sorted_data_list = Sort_Tuple(data_list, 1)\n",
    "            return sorted_data_list\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "def put(path, name, csv):\n",
    "    '''\n",
    "    places the file from a local directory into the EDFS\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        name (str): the name of the file to be created\n",
    "        csv (str): the path to the csv file to be placed\n",
    "    Returns:\n",
    "        output (str): the success or failure of the operation\n",
    "    '''\n",
    "    result = seek(path)\n",
    "    if result:\n",
    "        if result[0][1] == \"DIRECTORY\":\n",
    "            dup_result = seek(f\"{path}/{name}\")\n",
    "            if not dup_result:\n",
    "                hash_lists = hash(path, name, csv)\n",
    "                output = f\"file {name} created\"\n",
    "            else:\n",
    "                output = \"file already exists\"\n",
    "        else:\n",
    "            output = \"cannot place a file in a file\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def hash(path, name, csv_file):\n",
    "    '''\n",
    "    Alters the metadata to allocate new datanotes if needed and places the file\n",
    "    data into the nodes\n",
    "    Args:\n",
    "        path (str): the path to the file\n",
    "        name (str): the name of the file to be created\n",
    "        csv (str): the path to the csv file to be placed\n",
    "    Returns:\n",
    "        key_list (list): the list of keys to datanodes that have been\n",
    "        allocated to\n",
    "    '''\n",
    "\n",
    "    #execute metadata alter\n",
    "    meta_statement = \"INSERT INTO df VALUES (%s, 'FILE');\"\n",
    "    mycursor.execute(meta_statement, (f\"{path}/{name}\",))\n",
    "    mydb.commit()\n",
    "    with open(csv_file) as f:\n",
    "\n",
    "        key_list, csv_counter = {}, 0\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        header = next(reader)\n",
    "        key_index = key_idx(header)\n",
    "        key = key_cleaning(header)\n",
    "\n",
    "        #TODO: modularize this create code JFC\n",
    "        try:\n",
    "            create_statement = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {key}(\n",
    "                    path varchar(255),\n",
    "                    data_index int,\n",
    "                    data text,\n",
    "                    FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                )\"\"\"\n",
    "            insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "            mycursor.execute(create_statement)\n",
    "            mydb.commit()\n",
    "            mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(header)))\n",
    "            mydb.commit()\n",
    "            key_list[key] = None\n",
    "            csv_counter += 1\n",
    "        except:\n",
    "            output = f\"ERROR: {mycursor.statement}\"\n",
    "\n",
    "        for row in reader:\n",
    "            key = key_cleaning(row)\n",
    "            #try insert data into datanode\n",
    "            try:\n",
    "                create_statement = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {key}(\n",
    "                        path varchar(255),\n",
    "                        data_index int,\n",
    "                        data text,\n",
    "                        FOREIGN KEY(path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                    )\"\"\"\n",
    "                insert_hash_statement = f\"INSERT INTO {key} VALUES (%s, %s, %s);\"\n",
    "                mycursor.execute(create_statement)\n",
    "                mydb.commit()\n",
    "                mycursor.execute(insert_hash_statement, (path + \"/\" + name, csv_counter, ','.join(row)))\n",
    "                mydb.commit()\n",
    "                key_list[key] = None\n",
    "                csv_counter += 1\n",
    "            except:\n",
    "                output = f\"ERROR: {mycursor.statement}\"\n",
    "                rm(path, name)\n",
    "                # return output\n",
    "\n",
    "        #write data into datanodes\n",
    "        for key in key_list.keys():\n",
    "             block_statement = \"INSERT INTO blockLocations VALUES(%s, %s);\"\n",
    "             mycursor.execute(block_statement, (f\"{path}/{name}\", key))\n",
    "        mydb.commit()\n",
    "        return key_list\n",
    "\n",
    "######################\n",
    "# Database Functions #\n",
    "######################\n",
    "\n",
    "def delete(list):\n",
    "    '''\n",
    "    Drops all tables in the list from the edfs\n",
    "    Args:\n",
    "        list (list): the list of table names to drop\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        for item in list:\n",
    "            drop_table = f\"DROP TABLE {key}\"\n",
    "            mycursor.execute(drop_table)\n",
    "            mydb.commit()\n",
    "        return \"Dropped tables\"\n",
    "    except:\n",
    "        return \"Database drop error\"\n",
    "\n",
    "def new_env(edfs):\n",
    "    '''\n",
    "    Executes EDFS SQL setup queries\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystel to setup\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    env_statements = [\n",
    "            f\"CREATE DATABASE {edfs}\",\n",
    "            f\"USE {edfs}\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE df (\n",
    "                    path varchar(255),\n",
    "                    type varchar(255),\n",
    "                    PRIMARY KEY(path)\n",
    "                )\"\"\",\n",
    "            \"INSERT INTO df VALUES ('/root', 'DIRECTORY')\",\n",
    "            \"\"\"\n",
    "                CREATE TABLE blockLocations (\n",
    "                    path varchar(255),\n",
    "                    partition_name varchar(255),\n",
    "                    CONSTRAINT FOREIGN KEY (path) REFERENCES df(path) ON DELETE CASCADE\n",
    "                )\"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        for s in env_statements:\n",
    "            mycursor.execute(s)\n",
    "        mydb.commit()\n",
    "        return f\"{edfs} created\"\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "\n",
    "def delete_env(edfs):\n",
    "    '''\n",
    "    Drops the EDFS database entirely\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystel to drop\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        drop_database = f\"DROP DATABASE {edfs};\"\n",
    "        mycursor.execute(drop_database)\n",
    "        mydb.commit()\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} deleted\"\n",
    "\n",
    "def start_env(edfs):\n",
    "    '''\n",
    "    Uses EDFS database\n",
    "    Args:\n",
    "        edfs (str): the name of the EDFS filesystem to run\n",
    "    Returns:\n",
    "        (str): the success or failure of the operation\n",
    "    '''\n",
    "    try:\n",
    "        use_database = f\"USE {edfs}\"\n",
    "        mycursor.execute(use_database)\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} started\"\n",
    "\n",
    "# def test_edfs(argv):\n",
    "\n",
    "#     edfs = \"edfs\"\n",
    "\n",
    "#     #Testing\n",
    "#     if \"--delete\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#     elif \"--new\" in argv:\n",
    "#         print(new_env(edfs))\n",
    "#     elif \"--restart\" in argv:\n",
    "#         print(delete_env(edfs))\n",
    "#         print(new_env(edfs))\n",
    "#     else:\n",
    "#         print(start_env(edfs))\n",
    "#         print(mkdir(\"/root\", \"foo\"))\n",
    "#         print(mkdir(\"/root/foo\", \"bar\"))\n",
    "#         #todo: put check to make sure that the file source exists\n",
    "#         print(put(\"/root/foo\", \"data\", \"../datasets/sql-edfs/data.csv\"))\n",
    "#         print(cat(\"/root/foo/data\"))\n",
    "#         print(ls(\"/root/foo\"))\n",
    "#         print(rm(\"/root\", \"data\"))\n",
    "#         print(ls(\"/tree\"))\n",
    "\n",
    "\n",
    "#     # test_edfs(sys.argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs deleted'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edfs=\"edfs\"\n",
    "delete_env(edfs)\n",
    "# new_env(edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs created'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_env(edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs started'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_env(edfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/root/user', 'DIRECTORY')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(\"/root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put(mycursor, \"/root/foo\", \"data\", \"datasets/sql-edfs/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datasets\u001b[39m/\u001b[39msql\u001b[39m-\u001b[39medfs\u001b[39m/\u001b[39mCookingData\u001b[39m.\u001b[39mcsv\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "# import simplejson as sp\n",
    "import json\n",
    "import os\n",
    "\n",
    "#mongodb client setup\n",
    "client = MongoClient(\"mongodb://root:root@localhost:27017/\")\n",
    "db = client['edfs']\n",
    "#collection = db['name_of_collection']\n",
    "\n",
    "#python connector setup\n",
    "\n",
    "####################\n",
    "# API Functions    #\n",
    "####################\n",
    "\n",
    "def read_dataset(path):\n",
    "    df=pd.read_csv(path)\n",
    "    df=df.reset_index()\n",
    "    df_melted = df.melt(df.set_index('Country Name'))\n",
    "    new_columns = list()\n",
    "    columns = df_melted.columns.str.strip()\n",
    "    for c in columns:\n",
    "            new_columns.append(c.replace(\" \",\"_\"))\n",
    "\n",
    "    # change column names in dataframe\n",
    "    df_melted.columns = new_columns\n",
    "    df_melted.columns = df_melted.columns.astype(str)\n",
    "    #print(\"readDataset\", df_melted.astype({'Year':'int', 'Value': 'float'}))\n",
    "    return df_melted\n",
    "\n",
    "def seek(path):\n",
    "    #print(\"seek\", db.blockLocations.find_one({\"path\": path}))\n",
    "    return db.blockLocations.find_one({\"path\": path})\n",
    "\n",
    "def mkdir(path, name):\n",
    "    result = seek(path)\n",
    "    output = \"\"\n",
    "    #print(\"mkdir\", result)\n",
    "    if not result:\n",
    "        path = path + \"/\" + name\n",
    "        db.blockLocations.insert_one({\"Path\": path, \"type\" : 'DIRECTORY'})\n",
    "        output = f\"Directory created\"\n",
    "    else:\n",
    "        output = f\"Invalid path: {path}\"\n",
    "    #print(\"mkdir\",output)\n",
    "    return output\n",
    "\n",
    "def rm(path, name):\n",
    "    result = seek(path)\n",
    "    path =  f\"{path}/{name}\"\n",
    "    output = \"\"\n",
    "    if result:\n",
    "        db.blockLocations.insert_one({\"Path\": path, \"type\" : 'DIRECTORY'})\n",
    "        output = f\"{path} deleted\"\n",
    "    else:\n",
    "        output = \"invalid deletion\"\n",
    "    #print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def ls(path):\n",
    "    result = seek(path)\n",
    "    print(result)\n",
    "    if result:\n",
    "        if result[\"type\"] == \"FILE\":\n",
    "            output = \"Cannot run 'ls' on files\"\n",
    "        elif result[\"type\"] == \"DIRECTORY\":\n",
    "            #output1=db.df.find()\n",
    "            query = {\"path\": {\"$regex\": f\"{path}\",\"$options\" :'i'}}\n",
    "            output1=db.blockLocations.find(query)\n",
    "            final=[]\n",
    "            for o in output1:\n",
    "                final.append(o)\n",
    "    else:\n",
    "        final = f\"Invalid path: {path}\"\n",
    "    \n",
    "    if(len(final)==0):\n",
    "        return final\n",
    "    else:\n",
    "        return final\n",
    "\n",
    "def readPartition(inp,file,path):\n",
    "    #a=put(file,path)\n",
    "    path1= '/'+path+'/'+file+'/'+inp\n",
    "    X = db.blockLocations.find_one({\"path\":path1})\n",
    "    if(X):\n",
    "        return (\"FILE EXISTS\")\n",
    "    else:\n",
    "        return (\"FILE DOES NOT EXIST\")\n",
    "\n",
    "def cat(path1):\n",
    "    '''db.blockLocations.aggregate([\n",
    "  { \"$project\": { \"path\": { \"$concat\": [ \"$path\", \" - \", \"$type\" ] } } },\n",
    "  { \"$merge\": \"Concatenate\" }\n",
    "])'''\n",
    "    #path=location=path1+file\n",
    "    db.blockLocations.aggregate([\n",
    "    { \"$lookup\":\n",
    "        {\n",
    "           \"from\": \"df\",\n",
    "           \"localField\": \"path\",\n",
    "           \"foreignField\": \"location\",\n",
    "           \"as\": \"merge\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$merge\":\"Concatenate\"\n",
    "    }\n",
    "])\n",
    "    #return(db.blockLocations.find_one({\"path\":f\"path\"}))\n",
    "    return(\"Concatenated\")\n",
    "\n",
    "def put(path, name, csvf):\n",
    "    header = [ \"\\ufeffCountry Name\",\t\"Country Code\",\t\"Indicator Name\",\n",
    "    \t\"2003\",\t\"2004\",\t\"2005\",\t\"2006\",\t\"2007\",\t\"2008\",\t\"2009\",\t\n",
    "        \"2010\",\t\"2011\",\t\"2012\",\t\"2013\",\t\"2014\",\t\"2015\",\t\"2016\",\n",
    "        \"2017\",\t\"2018\",\t\"2019\",\t\"2020\",\t\"2021\"]\n",
    "    csvpath = path+csvf\n",
    "    csvfile = open( csvf, 'r')\n",
    "    reader = csv.DictReader( csvfile )\n",
    "    #print(reader)\n",
    "    for each in reader:\n",
    "        #print(each)\n",
    "        row={}\n",
    "        blockLoc = {}\n",
    "        for field in header:\n",
    "            row[field]=each[field]\n",
    "            blockLoc[\"path\"] = path+ \"/\"+ each[\"\\ufeffCountry Name\"]\n",
    "            row[\"location\"] = path+ \"/\"+ each[\"\\ufeffCountry Name\"]\n",
    "            blockLoc[\"type\"] = \"FILE\"\n",
    "        #print (row)\n",
    "        db.df.insert_one(row)\n",
    "        db.blockLocations.insert_one(blockLoc)\n",
    "    return(\"Inserted Data\")\n",
    "\n",
    "#session = db.getMongo().startSession( { readPreference: { mode: \"primary\" } } )\n",
    "\n",
    "\n",
    "######################\n",
    "# Database Functions #\n",
    "######################\n",
    "\n",
    "def delete(list):\n",
    "    try:\n",
    "        for item in list:\n",
    "            col=db[f\"item\"]\n",
    "            col.drop()\n",
    "        return \"Dropped tables\"\n",
    "    except:\n",
    "        return \"Database drop error\"\n",
    "\n",
    "def new_env(edfs):\n",
    "    try:\n",
    "        db = client['edfs']\n",
    "        db.blockLocations.insert_one({\"path\":'/root',\"type\":'DIRECTORY'})\n",
    "        return f\"{edfs} created\"\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "\n",
    "def delete_env(edfs):\n",
    "    try:\n",
    "        client.drop_database('edfs')\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} deleted\"\n",
    "\n",
    "def start_env(edfs):\n",
    "    try:\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client['edfs']\n",
    "    except:\n",
    "        return \"Database error\"\n",
    "    return  f\"{edfs} started\"\n",
    "\n",
    "def test_edfs(argv):\n",
    "\n",
    "    edfs = \"edfs\"\n",
    "\n",
    "    #Testing\n",
    "    if \"--delete\" in argv:\n",
    "        print(delete_env(edfs))\n",
    "    elif \"--new\" in argv:\n",
    "        print(new_env(edfs))\n",
    "    elif \"--restart\" in argv:\n",
    "        print(delete_env(edfs))\n",
    "        print(new_env(edfs))\n",
    "    else:\n",
    "        print(start_env(edfs))\n",
    "        print(mkdir(\"/root\", \"foo\"))\n",
    "        print(mkdir(\"/root/foo\", \"bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs deleted'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edfs = \"edfs\"\n",
    "delete_env(edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edfs created'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_env(edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Directory created'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkdir(\"root\", \"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put(path, name, csvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Invalid path: /root/user'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls(\"/root/user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inserted Data'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "put(\"/root/user\", \"\", \"datasets/Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/user/Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m read_dataset(\u001b[39m\"\u001b[39;49m\u001b[39m/root/user/Data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb Cell 20\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_dataset\u001b[39m(path):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     df\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danieldacosta/Documents/USC/dsci551/distributed-file-system/main.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     df_melted \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mmelt(df\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mCountry Name\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/user/Data'"
     ]
    }
   ],
   "source": [
    "read_dataset(\"/root/user/Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/sql-edfs/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(50, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Series Name</th>\n",
       "      <th>Series Code</th>\n",
       "      <th>1960 [YR1960]</th>\n",
       "      <th>1961 [YR1961]</th>\n",
       "      <th>1962 [YR1962]</th>\n",
       "      <th>1963 [YR1963]</th>\n",
       "      <th>1964 [YR1964]</th>\n",
       "      <th>1965 [YR1965]</th>\n",
       "      <th>...</th>\n",
       "      <th>2012 [YR2012]</th>\n",
       "      <th>2013 [YR2013]</th>\n",
       "      <th>2014 [YR2014]</th>\n",
       "      <th>2015 [YR2015]</th>\n",
       "      <th>2016 [YR2016]</th>\n",
       "      <th>2017 [YR2017]</th>\n",
       "      <th>2018 [YR2018]</th>\n",
       "      <th>2019 [YR2019]</th>\n",
       "      <th>2020 [YR2020]</th>\n",
       "      <th>2021 [YR2021]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Central Europe and the Baltics</td>\n",
       "      <td>CEB</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>99.8278344126722</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>99.9796400333302</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>ATG</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Haiti</td>\n",
       "      <td>HTI</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>37.9000015258789</td>\n",
       "      <td>38.6978759765625</td>\n",
       "      <td>39.496410369873</td>\n",
       "      <td>40.8362846374512</td>\n",
       "      <td>40.4000015258789</td>\n",
       "      <td>43.7492713928223</td>\n",
       "      <td>44.962345123291</td>\n",
       "      <td>45.990234375</td>\n",
       "      <td>46.9255332946777</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>31.8443844038242</td>\n",
       "      <td>31.794159992766</td>\n",
       "      <td>32.001026781227</td>\n",
       "      <td>33.8719104646129</td>\n",
       "      <td>38.8801732244203</td>\n",
       "      <td>40.261357595893</td>\n",
       "      <td>43.061876950924</td>\n",
       "      <td>44.2708604789338</td>\n",
       "      <td>45.8034852293869</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>MEX</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1116333007813</td>\n",
       "      <td>99.1476974487305</td>\n",
       "      <td>99.1729278564453</td>\n",
       "      <td>99</td>\n",
       "      <td>99.5</td>\n",
       "      <td>100</td>\n",
       "      <td>99.5</td>\n",
       "      <td>99.5999984741211</td>\n",
       "      <td>99.4000015258789</td>\n",
       "      <td>..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Country Name Country Code  \\\n",
       "208  Central Europe and the Baltics          CEB   \n",
       "6               Antigua and Barbuda          ATG   \n",
       "79                            Haiti          HTI   \n",
       "204     Africa Eastern and Southern          AFE   \n",
       "117                          Mexico          MEX   \n",
       "\n",
       "                                 Series Name     Series Code 1960 [YR1960]  \\\n",
       "208  Access to electricity (% of population)  EG.ELC.ACCS.ZS            ..   \n",
       "6    Access to electricity (% of population)  EG.ELC.ACCS.ZS            ..   \n",
       "79   Access to electricity (% of population)  EG.ELC.ACCS.ZS            ..   \n",
       "204  Access to electricity (% of population)  EG.ELC.ACCS.ZS            ..   \n",
       "117  Access to electricity (% of population)  EG.ELC.ACCS.ZS            ..   \n",
       "\n",
       "    1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964] 1965 [YR1965]  \\\n",
       "208            ..            ..            ..            ..            ..   \n",
       "6              ..            ..            ..            ..            ..   \n",
       "79             ..            ..            ..            ..            ..   \n",
       "204            ..            ..            ..            ..            ..   \n",
       "117            ..            ..            ..            ..            ..   \n",
       "\n",
       "     ...     2012 [YR2012]     2013 [YR2013]     2014 [YR2014]  \\\n",
       "208  ...               100               100               100   \n",
       "6    ...               100               100               100   \n",
       "79   ...  37.9000015258789  38.6978759765625   39.496410369873   \n",
       "204  ...  31.8443844038242   31.794159992766   32.001026781227   \n",
       "117  ...  99.1116333007813  99.1476974487305  99.1729278564453   \n",
       "\n",
       "        2015 [YR2015]     2016 [YR2016]     2017 [YR2017]    2018 [YR2018]  \\\n",
       "208               100  99.8278344126722               100              100   \n",
       "6                 100               100               100              100   \n",
       "79   40.8362846374512  40.4000015258789  43.7492713928223  44.962345123291   \n",
       "204  33.8719104646129  38.8801732244203   40.261357595893  43.061876950924   \n",
       "117                99              99.5               100             99.5   \n",
       "\n",
       "        2019 [YR2019]     2020 [YR2020] 2021 [YR2021]  \n",
       "208               100  99.9796400333302            ..  \n",
       "6                 100               100            ..  \n",
       "79       45.990234375  46.9255332946777            ..  \n",
       "204  44.2708604789338  45.8034852293869            ..  \n",
       "117  99.5999984741211  99.4000015258789            ..  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('datasets/Access_Electricity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/sql-edfs/CookingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.sample(50, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('datasets/Access_Fuels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Human_Capital_Index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.sample(50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"datasets/Human_Capital_Index_Sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed29fcc266c631214eb0a32dcb461d51a7279cf73c87ecfbb65b364d13f4dbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
